"""
í”„ë¡œì íŠ¸ ì‚¬ìš©ì— í•„ìš”í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ìŒ

"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from urllib.request import urlretrieve
helper_path = os.path.join(current_dir, "helper_c0z0c_dev.py")
urlretrieve("https://raw.githubusercontent.com/c0z0c/jupyter_hangul/refs/heads/beta/helper_c0z0c_dev.py", helper_path)

import importlib
import helper_c0z0c_dev as helper
importlib.reload(helper)
import seaborn as sns

# --- Scikit-learn: ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸, í‰ê°€ ---
from sklearn.linear_model import LinearRegression  # ì„ í˜•/ë‹¤ì¤‘ íšŒê·€
from sklearn.preprocessing import PolynomialFeatures, StandardScaler  # ë‹¤í•­ íŠ¹ì„±, ì •ê·œí™”
from sklearn.model_selection import train_test_split  # ë°ì´í„° ë¶„í• 
from sklearn.datasets import (
    fetch_california_housing, load_iris, make_moons, make_circles,
    load_breast_cancer, load_wine
)  # ë‹¤ì–‘í•œ ì˜ˆì œ ë°ì´í„°ì…‹
from sklearn import datasets  # ì¶”ê°€ ë°ì´í„°ì…‹
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree  # ê²°ì •íŠ¸ë¦¬
from sklearn.ensemble import RandomForestClassifier  # ëœë¤í¬ë ˆìŠ¤íŠ¸
from sklearn.metrics import accuracy_score, mean_squared_error  # í‰ê°€ ì§€í‘œ

# --- ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬

# --- PyTorch: ë”¥ëŸ¬ë‹ ê´€ë ¨ ---
import torch
import torch.nn as nn  # ì‹ ê²½ë§
import torch.optim as optim
import torch.nn.functional as F  # í™œì„±í™” í•¨ìˆ˜
from torch.utils.data import Dataset, DataLoader  # PyTorch ë°ì´í„°ì…‹/ë¡œë”
from torchvision.transforms import v2

# --- ê¸°íƒ€ ---
import os
import yaml
import requests
import tarfile
import shutil
import json
import signal
from datetime import datetime
from pathlib import Path
import re
from tqdm import tqdm
import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°
import matplotlib.pyplot as plt  # ì‹œê°í™”
import pandas as pd

# --- ë””ë°”ì´ìŠ¤ ì„¤ì • ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

################################################################################################################

def get_tqdm_kwargs():
    """Widget ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ì•ˆì „í•œ tqdm ì„¤ì •"""
    return {
        'disable': False,
        'leave': True,
        'file': sys.stdout,
        'ascii': True,  # ASCII ë¬¸ìë§Œ ì‚¬ìš©
        'dynamic_ncols': False,
#        'ncols': 80  # ê³ ì • í­
    }

def drive_root():
    root_path = os.path.join("D:\\", "GoogleDrive")
    if helper.is_colab:
        root_path = os.path.join("/content/drive/MyDrive")
    return root_path

def get_path_modeling(add_path = None):
    modeling_path = "modeling"
    path = os.path.join(drive_root(),modeling_path)
    if add_path is not None:
        path = os.path.join(path,add_path)
    return path

def get_path_modeling_release(add_path = None):
    modeling_path = "modeling_release"
    path = os.path.join(drive_root(),modeling_path)
    if add_path is not None:
        path = os.path.join(path,add_path)
    return path
    
################################################################################################################

def print_dir_tree(root, indent=""):
    """ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        root (str): ì‹œì‘ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        max_depth (int, optional): ìµœëŒ€ ê¹Šì´. Defaults to 2.
        indent (str, optional): ë“¤ì—¬ì“°ê¸° ë¬¸ìì—´. Defaults to "".
    """
    import os
    try:
        items = os.listdir(root)
    except Exception as e:
        print(indent + f"[Error] {e}")
        return

    img_count = len([f for f in os.listdir(root)])
    for item in items:
        path = os.path.join(root, item)
        if os.path.isdir(path):
            print(indent + "|-- "+ item)
            # ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜ë§Œ ì¶œë ¥
            img_count = len([f for f in os.listdir(path)])
            print(indent + "   "+ f"[ë°ì´í„°íŒŒì¼: {img_count}ê°œ]")
            print_dir_tree(root=path, indent=indent + "   ")
        else:
            print(indent + "|-- "+ item)
            

def print_json_tree(data, indent="", max_depth=4, _depth=0, list_count=2, print_value=True, limit_value_text=100):
    """
    JSON ê°ì²´ë¥¼ ì§€ì •í•œ ë‹¨ê³„(max_depth)ê¹Œì§€ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥
    - list íƒ€ì…ì€ 3ê°œ ì´ìƒì¼ ë•Œ ê°œìˆ˜ë§Œ ì¶œë ¥
    - í•˜ìœ„ ë…¸ë“œê°€ ê°’ì¼ ê²½ìš° key(type) í˜•íƒœë¡œ ì¶œë ¥
    - print_value=Trueì¼ ë•Œ key(type): ê°’ í˜•íƒœë¡œ ì¶œë ¥
    """
    if _depth > max_depth:
        return
    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                print(f"{indent}|-- {key}")
                print_json_tree(value, indent + "    ", max_depth, _depth + 1, list_count, print_value)
            else:
                if print_value:
                    print(f"{indent}|-- {key}({type(value).__name__}): {value if len(str(value)) < limit_value_text else f'{str(value)[:30]}...'}")
                else:
                    print(f"{indent}|-- {key}({type(value).__name__})")
    elif isinstance(data, list):
        if len(data) > list_count:
            print(f"{indent}|-- [list] ({len(data)} items)")
        else:
            for i, item in enumerate(data):
                if isinstance(item, (dict, list)):
                    print(f"{indent}|-- [{i}]")
                    print_json_tree(item, indent + "    ", max_depth, _depth + 1, list_count, print_value)
                else:
                    if print_value:
                        print(f"{indent}|-- [{i}]({type(item).__name__}): {item if len(str(item)) < limit_value_text else f'{str(item)[:30]}...'}")
                    else:
                        print(f"{indent}|-- [{i}]({type(item).__name__})")
    else:
        if print_value:
            print(f"{indent}{type(data).__name__}: {data if len(str(data)) < limit_value_text else f'{str(data)[:30]}...'}")
        else:
            print(f"{indent}{type(data).__name__}")

def print_git_tree(data, indent="", max_depth=3, _depth=0):
    """
    PyTorch tensor/ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ git tree ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥
    """
    import torch
    import numpy as np

    if _depth > max_depth:
        return
    if isinstance(data, dict):
        for key, value in data.items():
            print(f"{indent}â”œâ”€ {key} [{type(value).__name__}]")
            print_git_tree(value, indent + "â”‚  ", max_depth, _depth + 1)
    elif isinstance(data, (list, tuple)):
        for i, item in enumerate(data):
            print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
            print_git_tree(item, indent + "â”‚  ", max_depth, _depth + 1)
    elif torch.is_tensor(data):
        shape = tuple(data.shape)
        dtype = str(data.dtype)
        preview = str(data)
        preview_str = preview[:80] + ("..." if len(preview) > 80 else "")
        print(f"{indent}â””â”€ Tensor shape={shape} dtype={dtype} preview={preview_str}")
    elif isinstance(data, np.ndarray):
        shape = data.shape
        dtype = data.dtype
        preview = str(data)
        preview_str = preview[:80] + ("..." if len(preview) > 80 else "")
        print(f"{indent}â””â”€ ndarray shape={shape} dtype={dtype} preview={preview_str}")
    else:
        val_str = str(data)
        print(f"{indent}â””â”€ {type(data).__name__}: {val_str[:80]}{'...' if len(val_str)>80 else ''}")

################################################################################################################

def save_model_dict(model, path, pth_name, kwargs=None):
    """
    ëª¨ë¸ state_dictì™€ ì¶”ê°€ ì •ë³´ë¥¼ ì €ì¥
    """
    def safe_makedirs(path):
        """ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if os.path.exists(path) and not os.path.isdir(path):
            os.remove(path)  # íŒŒì¼ì´ë©´ ì‚­ì œ
        os.makedirs(path, exist_ok=True)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    safe_makedirs(path)

    # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
    model_info = {
        'class_name': model.__class__.__name__,
        'init_args': {},
        'str': str(model),
        'repr': repr(model),
        'modules': [m.__class__.__name__ for m in model.modules()],
    }

    # ìƒì„±ì ì¸ì ìë™ ì¶”ì¶œ(ê°€ëŠ¥í•œ ê²½ìš°)
    if hasattr(model, '__dict__'):
        for key in ['in_ch', 'base_ch', 'num_classes', 'out_ch']:
            if hasattr(model, key):
                model_info['init_args'][key] = getattr(model, key)

    # kwargs ì²˜ë¦¬
    extra_info = {}
    if kwargs is not None:
        if isinstance(kwargs, str):
            extra_info = json.loads(kwargs)
        elif isinstance(kwargs, dict):
            extra_info = kwargs

    model_info.update(extra_info)

    # ì €ì¥í•  dict êµ¬ì„±
    save_dict = {
        'model_state': model.state_dict(),
        'class_name': model.__class__.__name__,
        'model_info': model_info,
    }

    save_path = os.path.join(path, f"{pth_name}.pth")
    torch.save(save_dict, save_path)
    return save_path

def load_model_dict(path, pth_name=None):
    """
    save_model_dictë¡œ ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’: (model_state, model_info)
    """
    import torch
    load_path = path
    if pth_name is not None:
        load_path = os.path.join(path, f"{pth_name}.pth")
    checkpoint = torch.load(load_path, map_location='cpu', weights_only=False)  # <-- ì—¬ê¸° ì¶”ê°€
    model_state = checkpoint.get('model_state')
    model_info = checkpoint.get('model_info')
    model_info['file_name'] = os.path.basename(load_path)
    return model_state, model_info


def search_pth_files(base_path):
    """
    ì…ë ¥ëœ ê²½ë¡œì˜ í•˜ìœ„ í´ë”ë“¤ì—ì„œ pth íŒŒì¼ë“¤ì„ ê²€ìƒ‰
    """
    pth_files = []

    if not os.path.exists(base_path):
        print(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        return pth_files

    print(f"pth íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: {base_path}")

    # í•˜ìœ„ í´ë”ë“¤ì„ ìˆœíšŒí•˜ë©° pth íŒŒì¼ ê²€ìƒ‰
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth'):
                pth_path = os.path.join(root, file)
                pth_files.append(pth_path)

    # ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥
    if pth_files:
        print(f"\në°œê²¬ëœ pth íŒŒì¼ë“¤ ({len(pth_files)}ê°œ):")
        for i, pth_file in enumerate(pth_files, 1):
            # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ (base_path ê¸°ì¤€)
            rel_path = os.path.relpath(pth_file, base_path)
            print(f" {i:2d}. {rel_path}")
    else:
        print("pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return pth_files

################################################################################################################

class AIHubShell:
    def __init__(self, DEBUG=False, download_dir=None):
        self.BASE_URL = "https://api.aihub.or.kr"
        self.LOGIN_URL = f"{self.BASE_URL}/api/keyValidate.do"
        self.BASE_DOWNLOAD_URL = f"{self.BASE_URL}/down/0.5"
        self.MANUAL_URL = f"{self.BASE_URL}/info/api.do"
        self.BASE_FILETREE_URL = f"{self.BASE_URL}/info"
        self.DATASET_URL = f"{self.BASE_URL}/info/dataset.do"
        self.DEBUG = DEBUG
        self.download_dir = download_dir if download_dir else "."
                
    def help(self):
        """AIHubShell í´ë˜ìŠ¤ ì‚¬ìš©ë²• ì¶œë ¥"""
        print("=" * 80)
        print("                        AIHubShell í´ë˜ìŠ¤ ì‚¬ìš© ê°€ì´ë“œ")
        print("=" * 80)
        print()
        
        print("ğŸ”§ ì´ˆê¸°í™”")
        print("  AIHubShell(DEBUG=False, download_dir=None)")
        print("    DEBUG: Trueë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
        print("    download_dir: ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì • (ê¸°ë³¸ê°’: í˜„ì¬ ê²½ë¡œ)")
        print()
        
        print("ğŸ“‹ ë°ì´í„°ì…‹ ì¡°íšŒ")
        print("  .dataset_info()                    # ì „ì²´ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ")
        print("  .dataset_search('ê²€ìƒ‰ì–´')          # íŠ¹ì • ì´ë¦„ í¬í•¨ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  .dataset_search('ê²€ìƒ‰ì–´', tree=True) # ê²€ìƒ‰ + íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ")
        print("  .list_info(datasetkey=576)         # íŠ¹ì • ë°ì´í„°ì…‹ì˜ íŒŒì¼ ëª©ë¡")
        print("  .json_info(datasetkey=576)         # JSON í˜•íƒœë¡œ íŒŒì¼ êµ¬ì¡° ë°˜í™˜")
        print()
        
        print("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
        print("  .download_dataset(apikey, datasetkey, filekeys='all')")
        print("    apikey: AI Hub API í‚¤")
        print("    datasetkey: ë°ì´í„°ì…‹ ë²ˆí˜¸")
        print("    filekeys: íŒŒì¼í‚¤ ('all' ë˜ëŠ” '66065,66083' í˜•íƒœ)")
        print("    overwrite: ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ì—¬ë¶€ (ê¸°ë³¸ê°’: False)")
        print()
        
        print("ğŸ“– ê¸°íƒ€ ê¸°ëŠ¥")
        print("  .print_usage()                     # AI Hub API ìƒì„¸ ì‚¬ìš©ë²•")
        print("  .help()                            # ì´ ë„ì›€ë§")
        print()
        
        print("ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        print("  # 1. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        print("  aihub = AIHubShell(DEBUG=True, download_dir='./data')")
        print()
        print("  # 2. ê²½êµ¬ì•½ì œ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  aihub.dataset_search('ê²½êµ¬ì•½ì œ')")
        print()
        print("  # 3. ë°ì´í„°ì…‹ 576ì˜ íŒŒì¼ ëª©ë¡ í™•ì¸")
        print("  aihub.list_info(datasetkey=576)")
        print()
        print("  # 4. íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='66065,66083'")
        print("  )")
        print()
        print("  # 5. ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='all'")
        print("  )")
        print()
        
        print("âš ï¸  ì£¼ì˜ì‚¬í•­")
        print("  - API í‚¤ëŠ” AI Hubì—ì„œ ë°œê¸‰ë°›ì•„ì•¼ í•©ë‹ˆë‹¤")
        print("  - ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œ ì¶©ë¶„í•œ ì €ì¥ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”")
        print("  - overwrite=Falseì¼ ë•Œ ê¸°ì¡´ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤")
        print("  - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print()
        
        print("ğŸ” ì¶”ê°€ ì •ë³´")
        print("  AI Hub API ê³µì‹ ë¬¸ì„œ: https://aihub.or.kr")
        print("  ë¬¸ì œ ë°œìƒ ì‹œ DEBUG=Trueë¡œ ì„¤ì •í•˜ì—¬ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        print("=" * 80)
                        
    def print_usage(self):
        """ì‚¬ìš©ë²• ì¶œë ¥"""
        try:
            response = requests.get(self.MANUAL_URL)
            manual = response.text
            
            if self.DEBUG:
                print("API ì›ë³¸ ì‘ë‹µ:")
                print(manual)            
            
            # JSON íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            try:
                manual = re.sub(r'("FRST_RGST_PNTTM":)([0-9\- :\.]+)', r'\1"\2"', manual)
                manual_data = json.loads(manual)
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì„±ê³µ")
                    
                if 'result' in manual_data and len(manual_data['result']) > 0:
                    print(manual_data['result'][0].get('SJ', ''))
                    print()
                    print("ENGL_CMGG\t KOREAN_CMGG\t\t\t DETAIL_CN")
                    print("-" * 80)
                    
                    for item in manual_data['result']:
                        engl = item.get('ENGL_CMGG', '')
                        korean = item.get('KOREAN_CMGG', '')
                        detail = item.get('DETAIL_CN', '').replace('\\n', '\n').replace('\\t', '\t')
                        print(f"{engl:<10}\t {korean:<15}\t|\t {detail}\n")
            except json.JSONDecodeError:
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
                else:
                    print("API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜")
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
    
    def _merge_parts(self, target_dir):
        """part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        target_path = Path(target_dir)
        part_files = list(target_path.glob("*.part*"))
        
        if not part_files:
            return
            
        # prefixë³„ë¡œ ê·¸ë£¹í™”
        prefixes = {}
        for part_file in part_files:
            match = re.match(r'(.+)\.part(\d+)$', part_file.name)
            if match:
                prefix = match.group(1)
                part_num = int(match.group(2))
                if prefix not in prefixes:
                    prefixes[prefix] = []
                prefixes[prefix].append((part_num, part_file))
        
        # ê° prefixë³„ë¡œ ë³‘í•©
        for prefix, parts in prefixes.items():
            print(f"Merging {prefix} in {target_dir}")
            parts.sort(key=lambda x: x[0])  # part ë²ˆí˜¸ë¡œ ì •ë ¬
            
            output_path = target_path / prefix
            with open(output_path, 'wb') as output_file:
                for _, part_file in parts:
                    with open(part_file, 'rb') as input_file:
                        shutil.copyfileobj(input_file, output_file)
            
            # part íŒŒì¼ë“¤ ì‚­ì œ
            for _, part_file in parts:
                part_file.unlink()
                
    def _merge_parts_all(self, base_path="."):
        """ëª¨ë“  í•˜ìœ„ í´ë”ì˜ part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        if self.DEBUG:
            print("ë³‘í•© ì¤‘ì…ë‹ˆë‹¤...")
        for root, dirs, files in os.walk(base_path):
            part_files = [f for f in files if '.part' in f]
            if part_files:
                self._merge_parts(root)
        if self.DEBUG:
            print("ë³‘í•©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def download_dataset(self, apikey, datasetkey, filekeys="all", overwrite=False):
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ì˜µì…˜: ë®ì–´ì“°ê¸°)"""
        def _parse_size(size_str):
            """'92 GB', '8 MB' ë“± ë¬¸ìì—´ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ë³€í™˜"""
            size_str = size_str.strip().upper()
            if 'GB' in size_str:
                return float(size_str.replace('GB', '').strip()) * 1024**3
            elif 'MB' in size_str:
                return float(size_str.replace('MB', '').strip()) * 1024**2
            elif 'KB' in size_str:
                return float(size_str.replace('KB', '').strip()) * 1024
            elif 'B' in size_str:
                return float(size_str.replace('B', '').strip())
            return 0
        
        download_path = Path(self.download_dir)
        download_tar_path = download_path / "download.tar"
        
        download_list = self.list_info(datasetkey=datasetkey, filekeys=filekeys, print_out=False)
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ì œì™¸
        keys_to_download = []
        for key, info in download_list.items():
            extracted_file_path = os.path.join(self.download_dir, info.path)
            if not overwrite and os.path.exists(extracted_file_path):
                print(f"íŒŒì¼ ë°œê²¬: {extracted_file_path}")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue
            
            # ì••ì¶• í•´ì§€ í•˜ê³  ìš©ëŸ‰ ì´ìŠˆë¡œ ì¸í•˜ì—¬ zipíŒŒì¼ì€ ì‚­ì œ ë˜ì—ˆë‹¤.
            if not overwrite and os.path.exists(extracted_file_path + ".unzip"):
                print(f"íŒŒì¼ ë°œê²¬ unzip: {extracted_file_path}.unzip")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue
            
            keys_to_download.append(str(key))

        # ë‹¤ìš´ë¡œë“œí•  filekeysê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not keys_to_download:
            print("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            extracted_files = []
            for key, info in download_list.items():
                file_path = os.path.join(self.download_dir, info.path)
                if os.path.exists(file_path):
                    extracted_files.append(file_path)
            print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
            return extracted_files            

        # í—¤ë”ì™€ íŒŒë¼ë¯¸í„° ê¸°ë³¸ ì„¤ì •
        headers = {"apikey": apikey}
        params = {"fileSn": ",".join(keys_to_download)}
        
        mode = "wb"
        existing_size = 0
        response_head = requests.head(f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do", headers=headers, params=params)
        if "content-length" in response_head.headers:
            total_size = int(response_head.headers.get('content-length', 0))
        else:
            total_size = 0
            if self.DEBUG:
                print("content-length í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í¬ê¸° ì•Œ ìˆ˜ ì—†ìŒ.")
                print("HEAD ì‘ë‹µ í—¤ë”:", response_head.headers)

        if total_size == 0:
            total_size = int(sum(_parse_size(info.size) for info in download_list.values()))
            if self.DEBUG:
                print(f"download_list ê¸°ë°˜ ì¶”ì • total_size: {total_size / (1024**3):.2f} GB")
                
        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ
        if self.DEBUG:
            print("ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
        os.makedirs(download_path, exist_ok=True)
        response = requests.get(
            f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do",
            headers=headers,
            params=params,
            stream=True
        )

        if response.status_code in [200, 206]:
            
            with open(download_tar_path, mode) as f, tqdm(
                total=total_size, 
                unit='B', 
                unit_scale=True, 
                desc="Downloading", 
                mininterval=3.0,  # 3ì´ˆë§ˆë‹¤ ê°±ì‹ 
                initial=(existing_size if mode == "ab" else 0)
            ) as pbar:
                update_count = 1000
                downloaded = existing_size if mode == "ab" else 0
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    #f.flush()
                    downloaded += len(chunk)
                    pbar.update(len(chunk))
                    if update_count <= 0:
                        pbar.set_postfix_str(f"{downloaded / (1024**2):.2f}MB / {total_size / (1024**2):.2f}MB")
                        update_count = 1000
                    update_count -= 1
                f.flush()
            
            if self.DEBUG:
                print("ì••ì¶• í•´ì œ ì¤‘...")
            with tarfile.open(download_tar_path, "r") as tar:
                tar.extractall(path=download_path)
            self._merge_parts_all(download_path)
            download_tar_path.unlink()
            
            print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"Download failed with HTTP status {response.status_code}.")
            print("Error msg:")
            print(response.text)
            if download_tar_path.exists():
                download_tar_path.unlink()
                
        extracted_files = []
        for key, info in download_list.items():
            file_path = os.path.join(self.download_dir, info.path)
            if os.path.exists(file_path):
                extracted_files.append(file_path)
        print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
        return extracted_files            
                
    def list_info(self, datasetkey=None, filekeys="all", print_out=True):
        """ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´ ì¡°íšŒ (filekeys, íŒŒì¼ëª…, ì‚¬ì´ì¦ˆ ì¶œë ¥ ë° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)"""
        resjson = self.json_info(datasetkey=datasetkey)
        
        # íŒŒì¼ ì •ë³´ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬
        file_info_dict = {}
        
        def extract_files(structure):
            """ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
            for item in structure:
                if item["type"] == "file" and "filekey" in item:
                    filekey = int(item["filekey"])
                    file_info_dict[filekey] = {
                        "filekey": item["filekey"],
                        "filename": item["name"],
                        "size": item["size"],
                        "path": item["path"],
                        "deep": item["deep"]
                    }
                elif item["type"] == "directory" and "children" in item:
                    extract_files(item["children"])
        
        # JSON êµ¬ì¡°ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        extract_files(resjson["structure"])
        
        # filekeys ì²˜ë¦¬
        if filekeys == "all":
            filtered_files = file_info_dict
        else:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ filekeys íŒŒì‹±
            requested_keys = []
            for key in filekeys.split(','):
                try:
                    requested_keys.append(int(key.strip()))
                except ValueError:
                    continue
            
            # ìš”ì²­ëœ filekeyë§Œ í•„í„°ë§
            filtered_files = {k: v for k, v in file_info_dict.items() if k in requested_keys}
        
        # ì¶œë ¥
        if print_out:
            print(f"Dataset: {datasetkey}")
            print("=" * 80)
            print(f"{'FileKey':<8} {'Filename':<30} {'Size':<10} {'Path'}")
            print("-" * 80)
            
            for filekey, info in sorted(filtered_files.items()):
                print(f"{info['filekey']:<8} {info['filename']:<30} {info['size']:<10} {info['path']}")
            
            print(f"\nì´ {len(filtered_files)}ê°œ íŒŒì¼")
        
        # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (FileInfo ê°ì²´ í˜•íƒœë¡œ)
        class FileInfo:
            def __init__(self, filekey, filename, size, path, deep):
                self.filekey = filekey
                self.filename = filename
                self.size = size
                self.path = path
                self.deep = deep
            
            def __str__(self):
                return f"FileInfo(filekey={self.filekey}, filename='{self.filename}', size='{self.size}' , path='{self.path}', deep={self.deep})"
            
            def __repr__(self):
                return self.__str__()
        
        result_dict = {}
        for filekey, info in filtered_files.items():
            result_dict[filekey] = FileInfo(
                filekey=info["filekey"],
                filename=info["filename"],
                size=info["size"],
                path=info["path"],
                deep=info["deep"]
            )
        
        return result_dict
        
    # filepath: [ê²½êµ¬ì•½ì œ_ì´ë¯¸ì§€_ë°ì´í„°.ipynb](http://_vscodecontentref_/0)
    def dataset_info(self, datasetkey=None, datasetname=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ"""
        if datasetkey:
            filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"
            print("Fetching file tree structure...")
            try:
                response = requests.get(filetree_url)
                # ì¸ì½”ë”© ìë™ ê°ì§€
                response.encoding = response.apparent_encoding
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        else:
            print("Fetching dataset information...")
            try:
                response = requests.get(self.DATASET_URL)
                response.encoding = 'utf-8'
                #response.encoding = 'euc-kr'
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def dataset_search(self, datasetname=None, tree=False):
        """
        ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŠ¹ì • ì´ë¦„ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
        datasetname: ê²€ìƒ‰í•  ë°ì´í„°ì…‹ ì´ë¦„ (ë¶€ë¶„ ì¼ì¹˜)
        tree: Trueì´ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ë„ ì¡°íšŒ        
        """
        print("Fetching dataset information...")
        try:
            response = requests.get(self.DATASET_URL)
            response.encoding = 'utf-8'
            text = response.text
            if datasetname:
                # datasetnameì´ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì¶œë ¥
                lines = text.splitlines()
                for line in lines:
                    if datasetname in line:
                        #print(line)
                        # 576, ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°
                        num, name = line.split(',', 1)
                        # í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
                        if tree:
                            self.dataset_info(datasetkey=num.strip())
                        else:
                            print(line)
            else:
                print(text)
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def _get_depth_from_star_count(self, star_count, depth_mapping):
        """star_count ê°’ì„ ê¹Šì´(deep)ë¡œ ë³€í™˜"""
        if star_count not in depth_mapping:
            # ìƒˆë¡œìš´ star_count ê°’ì´ë©´ ë°°ì—´ì— ì¶”ê°€
            depth_mapping.append(star_count)
            # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            depth_mapping.sort()
        
        # ë°°ì—´ì—ì„œì˜ ì¸ë±ìŠ¤ê°€ ê¹Šì´
        return depth_mapping.index(star_count)

    def _json_line(self, line, json_obj, depth_mapping, path_stack, weight=0, deep=0):
        """íŒŒì¼ íŠ¸ë¦¬ì˜ í•œ ì¤„ì„ JSON êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±í•˜ì—¬ ì¶”ê°€"""
        # íŠ¸ë¦¬ êµ¬ì¡° ê¸°í˜¸ë¥¼ ëª¨ë‘ *ë¡œ ë³€ê²½
        line = line.replace("â”œâ”€", "â””â”€")
        line = line.replace("â”‚ ", "â””â”€")
        while "    â””â”€" in line:
            line = line.replace("    â””â”€", "â””â”€â””â”€")
        while " â””â”€" in line:
            line = line.replace(" â””â”€", "â””â”€")
        
        while "â””â”€" in line:
            line = line.replace("â””â”€", "*")
        
        # ì•ë¶€ë¶„ì˜ * ê°œìˆ˜ì™€ ë¬¸ìì—´ ì¶”ì¶œ
        star_count = 0
        for char in line:
            if char == '*':
                star_count += 1
            else:
                break
        clean_str = line.replace('*', '').strip()
        
        # star_countë¥¼ deepë¡œ ë™ì  ë³€í™˜
        deep = self._get_depth_from_star_count(star_count, depth_mapping)
        
        has_pipe = "|" in line
        
        # íŒŒì¼/í´ë” ì •ë³´ ì¶”ì¶œ
        if has_pipe:
            parts = clean_str.split('|')
            if len(parts) >= 3:
                filename = parts[0].strip()
                size = parts[1].strip()
                filekey = parts[2].strip()
                item_type = "file"
            else:
                filename = clean_str
                size = ""
                filekey = ""
                item_type = "directory"
        else:
            filename = clean_str
            size = ""
            filekey = ""
            item_type = "directory"
        
        # path_stack ì¡°ì • (í˜„ì¬ ê¹Šì´ì— ë§ê²Œ)
        while len(path_stack) > deep:
            path_stack.pop()
        
        # í˜„ì¬ ì•„ì´í…œ ì •ë³´
        current_item = {
            "name": filename,
            "type": item_type,
            "deep": deep,
            "weight": star_count,
            "path": str(Path(*path_stack, filename)).replace(' ', '_')  # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        }
        
        if item_type == "file":
            current_item["size"] = size
            current_item["filekey"] = filekey
        else:
            current_item["children"] = []
        
        # JSON êµ¬ì¡°ì— ì¶”ê°€ (ë°°ì—´ êµ¬ì¡°)
        current_array = json_obj
        for path_name in path_stack:
            # í•´ë‹¹ ì´ë¦„ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ì„œ ê·¸ children ë°°ì—´ë¡œ ì´ë™
            found = None
            for item in current_array:
                if item["name"] == path_name and item["type"] == "directory":
                    found = item
                    break
            if found:
                current_array = found["children"]
        
        # í˜„ì¬ ë°°ì—´ì— ì•„ì´í…œ ì¶”ê°€
        current_array.append(current_item)
        
        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° path_stackì— ì¶”ê°€
        if item_type == "directory":
            path_stack.append(filename)
        
        # if self.DEBUG:
        #     print(f"[deep={deep}] [weight={star_count}] {item_type[0].upper()} {filename}" + 
        #         (f" , {size} , {filekey}" if item_type == "file" else " , , "))
        
        return current_item

    def json_info(self, datasetkey=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜"""
        filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"        
        response = requests.get(filetree_url)
        response.encoding = response.apparent_encoding
        text = response.text
        
        # JSON êµ¬ì¡°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        result = {
            "datasetkey": datasetkey,
            "structure": []  # ë°°ì—´ë¡œ ë³€ê²½
        }
        
        lines = text.splitlines()
        
        is_notify = True
        json_obj = []  # ë£¨íŠ¸ ë°°ì—´
        depth_mapping = []  # ê° íŒŒì‹± ì„¸ì…˜ë§ˆë‹¤ ìƒˆë¡œìš´ depth_mapping
        path_stack = []     # í˜„ì¬ ê²½ë¡œë¥¼ ì¶”ì í•˜ëŠ” ìŠ¤íƒ

        # if self.DEBUG:
        #     test_count = 10

        for line in lines:
            if not line.strip() or 'ê³µì§€ì‚¬í•­' in line or '=' in line:
                is_notify = False
                continue
            if is_notify:
                continue

            self._json_line(line, json_obj, depth_mapping, path_stack, weight=0, deep=0)

            # if self.DEBUG:
            #     test_count -= 1
            #     if test_count <= 0:
            #         break
        
        result["structure"] = json_obj
        
        return result

import zipfile
def unzip(zipfile_list):
    for zip_path in zipfile_list:
        if os.path.exists(zip_path) and os.path.isfile(zip_path):
            extract_dir = zip_path + ".unzip"
            if not os.path.exists(extract_dir):
                os.makedirs(extract_dir, exist_ok=True)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                print(f"ì••ì¶• í•´ì œ ì™„ë£Œ: {extract_dir}")
            else:
                print(f"ì´ë¯¸ ì••ì¶• í•´ì œë¨: {extract_dir}")
            try:
                os.remove(zip_path)
            except FileNotFoundError:
                pass

################################################################################################################
