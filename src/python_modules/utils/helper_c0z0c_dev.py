"""
Jupyter/Colab í•œê¸€ í°íŠ¸ ë° pandas í™•ì¥ ëª¨ë“ˆ

ğŸš€ ê¸°ë³¸ ì‚¬ìš©ë²•:
    import helper_c0z0c_dev as helper
    helper.setup()  # í•œë²ˆì— ëª¨ë“  ì„¤ì • ì™„ë£Œ

ğŸ“‚ ê°œë³„ ì‹¤í–‰:
    helper.font_download()      # í°íŠ¸ ë‹¤ìš´ë¡œë“œ
    helper.load_font()          # í°íŠ¸ ë¡œë”©
    helper.set_pandas_extension()  # pandas í™•ì¥ ê¸°ëŠ¥

ğŸ“Š íŒŒì¼ ì½ê¸°:
    df = helper.pd_read_csv("íŒŒì¼ëª….csv")          # ë¬¸ìì—´ ê²½ë¡œ (ìë™ ë³€í™˜)
    df = helper.pd_read_csv(file_obj, encoding='utf-8')  # íŒŒì¼ ê°ì²´/URL ë“±

ğŸ”§ ìœ í‹¸ë¦¬í‹°:
    helper.dir_start(ê°ì²´, "ì ‘ë‘ì‚¬")  # ë©”ì„œë“œ ê²€ìƒ‰
    df.head_att()  # í•œê¸€ ì»¬ëŸ¼ ì„¤ëª… ì¶œë ¥

ğŸ’¾ ìºì‹œ ê¸°ëŠ¥:
    key = helper.cache_key("model", params, random_state=42)  # í‚¤ ìƒì„±
    helper.cache_save(key, model)                           # ëª¨ë¸ ì €ì¥
    model = helper.cache_load(key)                          # ëª¨ë¸ ë¡œë“œ
    helper.cache_exists(key)                                # í‚¤ ì¡´ì¬ í™•ì¸
    helper.cache_info()                                     # ìºì‹œ ì •ë³´
    helper.cache_clear()                                    # ìºì‹œ ì´ˆê¸°í™”

ğŸ“ˆ pandas commit ì‹œìŠ¤í…œ:
    df.commit("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")              # DataFrame ìƒíƒœ ì €ì¥
    df_list = pd.DataFrame.commit_list()        # ì»¤ë°‹ ëª©ë¡ ì¡°íšŒ
    df_restored = pd.DataFrame.checkout(0)      # íŠ¹ì • ì»¤ë°‹ìœ¼ë¡œ ë³µì›
    pd.DataFrame.commit_rm("ë©”ì‹œì§€")            # ì»¤ë°‹ ì‚­ì œ

ğŸŒ AI Hub ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ:
    from helper_c0z0c_dev import AIHubShell
    aihub = AIHubShell()
    aihub.list_search(datasetname='ê²€ìƒ‰ì–´')     # ë°ì´í„°ì…‹ ê²€ìƒ‰
    aihub.download_dataset(apikey, datasetkey)  # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

ğŸ†• v2.5.0 ê°œì„ ì‚¬í•­:
    - AI Hub ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€

ì‘ì„±ì: ê¹€ëª…í™˜
ë‚ ì§œ: 2025.09.08
ë²„ì „: 2.5.0
ë¼ì´ì„¼ìŠ¤: MIT
"""

# =============================================================================
# IMPORTS
# =============================================================================

# =============================================================================
# IMPORTS
# =============================================================================

# Standard library imports
import os
import sys
import time
import json
import gzip
import shutil
import pickle
import hashlib
import warnings
import subprocess
import urllib.request
import signal
import re
from pathlib import Path
from datetime import datetime
import tarfile

# Third-party imports
import matplotlib.font_manager
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Environment-specific imports (handled with try/except in functions)
try:
    import IPython
    from IPython.display import HTML
    IPYTHON_AVAILABLE = True
except ImportError:
    IPYTHON_AVAILABLE = False

try:
    import google.colab
    from google.colab import drive
    COLAB_AVAILABLE = True
except ImportError:
    COLAB_AVAILABLE = False


# =============================================================================
# CONSTANTS AND GLOBAL VARIABLES
# =============================================================================

__version__ = "2.5.0"

# Font management
__font_path = ""
is_colab = False

# Pandas commit system
__COMMIT_META_FILE = "pandas_df.json"
__pd_root_base = None
__last_setup_time = None  # ëª¨ë“ˆ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ (ì¶œë ¥ ë©”ì‹œì§€ ì»¨íŠ¸ë¡¤)
__is_setup_print_log = False

# __DEBUG_ON = True
__DEBUG_ON = False

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================
def _in_colab():
    """Colab í™˜ê²½ ê°ì§€"""
    return COLAB_AVAILABLE

def _get_text_width(text):
    """í…ìŠ¤íŠ¸ í­ ê³„ì‚° (í•œê¸€ 2ì¹¸, ì˜ë¬¸ 1ì¹¸)"""
    if text is None:
        return 0
    return sum(2 if ord(char) >= 0x1100 else 1 for char in str(text))

def _format_value(value):
    """ê°’ì„ í¬ë§·íŒ…í•©ë‹ˆë‹¤. ì‹¤ìˆ˜í˜•ì€ ì†Œìˆ˜ì  ì´í•˜ 4ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼"""
    try:
        # ë°°ì—´ì´ë‚˜ ì‹œë¦¬ì¦ˆì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
        if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
            return str(value)
        
        # pandas NA ì²´í¬ (ìŠ¤ì¹¼ë¼ ê°’ì—ë§Œ ì ìš©)
        if pd.isna(value):
            return str(value)
        elif isinstance(value, (int, np.integer)):
            return str(value)
        elif isinstance(value, (float, np.floating)):
            return f"{value:.4f}".rstrip('0').rstrip('.')
        else:
            return str(value)
    except (ValueError, TypeError):
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
        return str(value)

def font_download():
    """í°íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ì„¤ì¹˜í•©ë‹ˆë‹¤."""
    global __font_path
    
    # matplotlib ê²½ê³  ì–µì œ
    warnings.filterwarnings(action='ignore')
    
    if _in_colab():
        # ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if os.system("dpkg -l | grep fonts-nanum") == 0:
            if __DEBUG_ON:
                print("âœ… Colabì— ë‚˜ëˆ” í°íŠ¸ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
            
        try:
            # ë‚˜ëˆ” í°íŠ¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ìºì‹œ ì—…ë°ì´íŠ¸ (ì¶œë ¥ ìµœì†Œí™”)
            print("install fonts-nanum...")
            subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-nanum', "-qq"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            subprocess.run(['sudo', 'fc-cache', '-fv'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            subprocess.run(['rm', '-rf', os.path.expanduser('~/.cache/matplotlib')], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)            
            
            if __DEBUG_ON:
                print("âœ… Colabì— ë‚˜ëˆ” í°íŠ¸ ì„¤ì¹˜ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í°íŠ¸ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            return False
    else:
        font_url = "https://github.com/c0z0c/jupyter_hangul/raw/master/NanumGothic.ttf"
        font_dir = "fonts"
        os.makedirs(font_dir, exist_ok=True)
        __font_path = os.path.join(font_dir, "NanumGothic.ttf")
        
        if not os.path.exists(__font_path):
            if __DEBUG_ON:
                print("ğŸ”½ ë¡œì»¬ì— ë‚˜ëˆ” í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")           
            urllib.request.urlretrieve(font_url, __font_path)
            if __DEBUG_ON:
                print("ğŸ”½ ë¡œì»¬ì— ë‚˜ëˆ” í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return True

def _colab_font_reinstall():
    """Colabì—ì„œ í°íŠ¸ ì¬ì„¤ì¹˜"""
    # matplotlib ê²½ê³  ì–µì œ
    warnings.filterwarnings(action='ignore')
    
    print("ğŸ”„ í°íŠ¸ ë¬¸ì œ ê°ì§€ - helper.setup() ì¬ì‹¤í–‰ ê¶Œì¥")
    
    try:
        # ìºì‹œ ì •ë¦¬ ë° íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜ (ì¶œë ¥ ì—†ì´)
        subprocess.run(['sudo', 'apt-get', 'remove', '--purge', '-y', 'fonts-nanum'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-nanum', "-qq"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(['sudo', 'fc-cache', '-fv'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(['rm', '-rf', os.path.expanduser('~/.cache/matplotlib')], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)            
        time.sleep(1)
        os.kill(os.getpid(), 9)
    except Exception:
        pass

def reset_matplotlib():
    """matplotlib ì™„ì „ ë¦¬ì…‹ (NumPy í˜¸í™˜ì„± ê°œì„ )"""
    # matplotlib ëª¨ë“ˆë“¤ì„ sys.modulesì—ì„œ ì œê±°
    
    if __DEBUG_ON:
        print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘... (helper v{__version__})")

    modules_to_remove = [mod for mod in sys.modules if mod.startswith('matplotlib')]
    for mod in modules_to_remove:
        del sys.modules[mod]

    if __DEBUG_ON:
        print("âœ… matplotlib ëª¨ë“ˆ ì œê±° ì™„ë£Œ")
    
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm


    if __DEBUG_ON:
        print("âœ… matplotlib ë‹¤ì‹œ ë¡œë“œ ì™„ë£Œ")
    
    # í°íŠ¸ ìºì‹œ í´ë¦¬ì–´ (ì¤‘ìš”!)
    try:
        fm._get_fontconfig_fonts.cache_clear()
    except:
        pass
    
    try:
        fm.fontManager.__init__()
    except:
        pass
    
    # í™˜ê²½ë³„ í°íŠ¸ ì„¤ì •
    if _in_colab():
        plt.rcParams['font.family'] = 'NanumBarunGothic'
    else:
        global __font_path
        if __font_path and os.path.exists(__font_path):
            fm.fontManager.addfont(__font_path)
            plt.rcParams['font.family'] = 'NanumGothic'
        else:
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            korean_fonts = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Noto Sans CJK KR']
            
            for font in korean_fonts:
                if font in available_fonts:
                    plt.rcParams['font.family'] = font
                    if __DEBUG_ON:
                        print(f"âœ… {font} í°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
            else:
                plt.rcParams['font.family'] = 'DejaVu Sans'
                print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. font_download()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 10
    
    # IPython í™˜ê²½ì—ì„œ ì „ì—­ ë“±ë¡ (Jupyter/Colab í˜¸í™˜ì„± ê°œì„ )
    try:
        if IPYTHON_AVAILABLE:
            ipy = IPython.get_ipython()
            if ipy is not None:
                ipy.user_ns["plt"] = plt
            else:
                globals()["plt"] = plt
        else:
            globals()["plt"] = plt
    except Exception:
        globals()["plt"] = plt
    
    return plt

def load_font():
    """í°íŠ¸ë¥¼ ë¡œë”©í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
    global __font_path, is_colab

    try:
        # matplotlib ê²½ê³  ì–µì œ
        warnings.filterwarnings(action='ignore')
        
        if _in_colab():
            is_colab = True
            
            # Google Drive ë§ˆìš´íŠ¸ ì‹œë„ (ì¶œë ¥ ì—†ì´)
            try:
                if COLAB_AVAILABLE:
                    drive.mount("/content/drive", force_remount=True)
            except Exception:
                pass
            
            
            # í•œê¸€ í°íŠ¸ê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            current_font = plt.rcParams.get('font.family', ['default'])
            if isinstance(current_font, list):
                current_font = current_font[0] if current_font else 'default'
            
            if 'nanum' in current_font.lower() or 'gothic' in current_font.lower():
                if __DEBUG_ON:
                    print("âœ… Colabì— í•œê¸€ í°íŠ¸ê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return True
            
            # í°íŠ¸ ì„¤ì • ì‹œë„ (ì¶œë ¥ ìµœì†Œí™”)
            try:
                reset_matplotlib()
                return True
                    
            except Exception as font_error:
                _colab_font_reinstall()
                return False
            
        else:
            is_colab = False
            current_font = plt.rcParams.get("font.family", "default")
            if isinstance(current_font, list):
                current_font = current_font[0] if current_font else "default"
                
            if current_font == "NanumGothic":
                return True

            try:
                if __font_path and os.path.exists(__font_path):
                    reset_matplotlib()
                    return True
                else:
                    return False
            except Exception as e:
                return False
                
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        
        if _in_colab():
            _colab_font_reinstall()
        else:
            print("ğŸ’¡ helper.font_download()ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
        
        return False

# pandas ì˜µì…˜ ì„¤ì •
pd.set_option("display.max_rows", 30)
pd.set_option("display.max_columns", 100)


# =============================================================================
# FILE I/O FUNCTIONS
# =============================================================================

def pd_read_csv(filepath_or_buffer, **kwargs):
    """
    Colab/ë¡œì»¬ í™˜ê²½ì— ë§ì¶° CSV íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    
    Parameters:
    -----------
    filepath_or_buffer : str, path object, file-like object
        ì½ì–´ì˜¬ íŒŒì¼ ê²½ë¡œ, URL, íŒŒì¼ ê°ì²´ ë“± (pd.read_csvì™€ ë™ì¼)
        - str íƒ€ì…ì´ê³  ë¡œì»¬ íŒŒì¼ ê²½ë¡œì¼ ê²½ìš°: Colab í™˜ê²½ì—ì„œ ìë™ìœ¼ë¡œ ê²½ë¡œ ë³€í™˜
        - URL (http://, https://, ftp://, file://): ê·¸ëŒ€ë¡œ pd.read_csvì— ì „ë‹¬
        - ë‹¤ë¥¸ íƒ€ì…ì¼ ê²½ìš°: ê·¸ëŒ€ë¡œ pd.read_csvì— ì „ë‹¬
    **kwargs : dict
        pd.read_csvì˜ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë“¤
    
    Returns:
    --------
    pandas.DataFrame : ì½ì–´ì˜¨ ë°ì´í„°í”„ë ˆì„
    
    Examples:
    ---------
    >>> # ë¡œì»¬ íŒŒì¼ (í™˜ê²½ë³„ ìë™ ë³€í™˜)
    >>> df = helper.pd_read_csv('data.csv')
    >>> 
    >>> # URL (ê·¸ëŒ€ë¡œ ì „ë‹¬)
    >>> df = helper.pd_read_csv('https://example.com/data.csv')
    >>> 
    >>> # íŒŒì¼ ê°ì²´ (ê·¸ëŒ€ë¡œ ì „ë‹¬)
    >>> with open('data.csv') as f:
    >>>     df = helper.pd_read_csv(f)
    """
    # ë¬¸ìì—´ ê²½ë¡œì¼ ê²½ìš°ì—ë§Œ ê²½ë¡œ ë³€í™˜ ì²˜ë¦¬ (URL ì œì™¸)
    if isinstance(filepath_or_buffer, str) and not filepath_or_buffer.startswith(('http://', 'https://', 'ftp://', 'file://')):
        # __pd_root_base/pd_root ì •ì±… ì ìš©
        full_path = os.path.join(pd_root(), filepath_or_buffer) if not os.path.isabs(filepath_or_buffer) else filepath_or_buffer
        try:
            if not os.path.exists(full_path):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
                return None
            df = pd.read_csv(full_path, **kwargs)
            print(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
            return df
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return None
    else:
        # ë¬¸ìì—´ì´ ì•„ë‹ˆê±°ë‚˜ URLì¸ ê²½ìš° (íŒŒì¼ ê°ì²´, URL ë“±) ê·¸ëŒ€ë¡œ ì „ë‹¬
        try:
            df = pd.read_csv(filepath_or_buffer, **kwargs)
            print(f"âœ… ë°ì´í„° ì½ê¸° ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
            return df
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return None

def dir_start(obj, cmd):
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ ë„ì›€ë§ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print('def dir_start(obj, cmd):')
    print('  for c in [att for att in dir(obj) if att.startswith(cmd)]:')
    print('    print(f"{c}")')
    print()
    for c in [att for att in dir(obj) if att.startswith(cmd)]:
        print(f"{c}")

def set_pd_root_base(subdir=None):
    """
    pd_rootì˜ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì§€ì†ì ìœ¼ë¡œ ì˜í–¥ì„ ì¤ë‹ˆë‹¤.
    - subdirì´ Noneì´ë©´: Colabì€ /content/drive/MyDrive, JupyterëŠ” í˜„ì¬ í´ë”
    - subdirì´ ë¬¸ìì—´ì´ë©´: Colabì€ /content/drive/MyDrive/subdir, JupyterëŠ” ./subdir
    - subdirì´ '/'ë¡œ ì‹œì‘í•˜ë©´: Colabì€ /content/drive/MyDrive/ + subdir, JupyterëŠ” . + subdir
    """
    if _in_colab():
        base = "/content/drive/MyDrive"
        if subdir is None or subdir == "":
            __pd_root_base = base
        elif subdir.startswith("/"):
            __pd_root_base = base + subdir
        else:
            __pd_root_base = os.path.join(base, subdir)
    else:
        base = "."
        if subdir is None or subdir == "":
            __pd_root_base = base
        elif subdir.startswith("/"):
            __pd_root_base = base + subdir
        else:
            __pd_root_base = os.path.join(base, subdir)

def pd_root(commit_dir=None):
    """
    pandas commit ì‹œìŠ¤í…œì˜ ê¸°ë³¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    commit_dirì´ ì§€ì •ë˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼, ì—†ìœ¼ë©´ pd_root_baseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if commit_dir is not None:
        return os.path.abspath(commit_dir)
    if __pd_root_base is not None:
        return os.path.abspath(__pd_root_base)
    # ê¸°ë³¸ê°’ ì„¤ì •
    if _in_colab():
        return "/content/drive/MyDrive"
    else:
        return os.path.abspath(".")

def _load_commit_meta(commit_dir=None):
    """ì»¤ë°‹ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    meta_file = os.path.join(os.path.join(pd_root(commit_dir), ".commit_pandas"), __COMMIT_META_FILE)
    if os.path.exists(meta_file):
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            return []
    return []

def _save_commit_meta(meta, commit_dir=None):
    """ì»¤ë°‹ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    meta_file = os.path.join(os.path.join(pd_root(commit_dir), ".commit_pandas"), __COMMIT_META_FILE)
    os.makedirs(os.path.dirname(meta_file), exist_ok=True)
    with open(meta_file, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

def _generate_commit_hash(dt, msg):
    """ì»¤ë°‹ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    base = f"{dt.strftime('%Y%m%d_%H%M%S')}_{msg}"
    return hashlib.md5(base.encode("utf-8")).hexdigest()[:12]


# =============================================================================
# PANDAS EXTENSION FUNCTIONS
# =============================================================================

def set_pandas_extension():
    """pandas DataFrame/Seriesì— í•œê¸€ ì»¬ëŸ¼ ì„¤ëª… ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
    # ê¸°ë³¸ ê¸°ëŠ¥
    for cls in [pd.DataFrame, pd.Series]:
        setattr(cls, "set_head_att", set_head_att)
        setattr(cls, "get_head_att", get_head_att)
        setattr(cls, "remove_head_att", remove_head_att)
        setattr(cls, "clear_head_att", clear_head_att)
        setattr(cls, "clear_head_ext", clear_head_ext)
    
    # DataFrame/Seriesë³„ ì¶œë ¥ í•¨ìˆ˜
    setattr(pd.DataFrame, "head_att", pd_head_att)
    setattr(pd.DataFrame, "_print_head_att", _print_head_att)
    setattr(pd.DataFrame, "_html_head_att", _html_head_att)
    setattr(pd.DataFrame, "_string_head_att", _string_head_att)
    setattr(pd.DataFrame, "_init_column_attrs", _init_column_attrs)
    setattr(pd.DataFrame, "_convert_columns", _convert_columns)
    setattr(pd.DataFrame, "_update_column_descriptions", _update_column_descriptions)
    setattr(pd.DataFrame, "_set_head_ext_bulk", _set_head_ext_bulk)
    setattr(pd.DataFrame, "_set_head_ext_individual", _set_head_ext_individual)
    setattr(pd.Series, "head_att", series_head_att)
    
    # ì»¬ëŸ¼ ì„¸íŠ¸ ê´€ë¦¬ ê¸°ëŠ¥
    for cls in [pd.DataFrame, pd.Series]:
        setattr(cls, "set_head_ext", set_head_ext)
        setattr(cls, "set_head_column", set_head_column)
        setattr(cls, "get_current_column_set", get_current_column_set)
        setattr(cls, "get_head_ext", get_head_ext)
        setattr(cls, "list_head_ext", list_head_ext)
        setattr(cls, "clear_head_ext", clear_head_ext)
        setattr(cls, "remove_head_ext", remove_head_ext)
    
    # Seriesì—ë„ ìƒˆ í•¨ìˆ˜ë“¤ ì¶”ê°€
    setattr(pd.Series, "_set_head_ext_bulk", _set_head_ext_bulk)
    setattr(pd.Series, "_set_head_ext_individual", _set_head_ext_individual)
    setattr(pd.Series, "_init_column_attrs", _init_column_attrs)
    setattr(pd.Series, "_convert_columns", _convert_columns)
    setattr(pd.Series, "_update_column_descriptions", _update_column_descriptions)

    # pandas commit ì‹œìŠ¤í…œ API ë°”ì¸ë”©
    setattr(pd.DataFrame, "commit", _df_commit)
    setattr(pd.DataFrame, "checkout", classmethod(_df_checkout))
    setattr(pd.DataFrame, "commit_list", classmethod(_df_commit_list))
    setattr(pd.DataFrame, "commit_rm", classmethod(_df_commit_rm))
    setattr(pd.DataFrame, "commit_has", classmethod(_df_commit_has))

# =============================================================================
# FONT MANAGEMENT FUNCTIONS
# =============================================================================

def _check_numpy_compatibility():
    """NumPy ë²„ì „ í˜¸í™˜ì„± ì²´í¬"""
    try:
        major_version = int(np.__version__.split('.')[0])
        minor_version = int(np.__version__.split('.')[1])
        
        if major_version >= 2:
            print(f"â„¹ï¸ NumPy {np.__version__} (v2.x+): í˜¸í™˜ì„± ëª¨ë“œ ì ìš©ë¨")
        elif major_version == 1 and minor_version < 20:
            print(f"âš ï¸ NumPy {np.__version__}: êµ¬ ë²„ì „ ê°ì§€, ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ ê°€ëŠ¥")
        
        return True
    except Exception:
        return False


# =============================================================================
# MAIN SETUP FUNCTION
# =============================================================================
def setup():
    """í•œë²ˆì— ëª¨ë“  ì„¤ì • ì™„ë£Œ"""
    global __pd_root_base
    global __last_setup_time
    global __is_setup_print_log
    
    if __DEBUG_ON:
        print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘... (helper v{__version__})")
        if __last_setup_time is not None:
            print(f"   - __last_setup_time : {datetime.datetime.fromtimestamp(__last_setup_time).strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            print(f"   - __last_setup_time : None")

    now = time.time()
    __is_setup_print_log = True
    if __last_setup_time is not None:
        elapsed = now - __last_setup_time
        if elapsed < 2.0:
            __is_setup_print_log = False
        else:
            __is_setup_print_log = True
            __last_setup_time = now
    else:
        __is_setup_print_log = True
        __last_setup_time = now
    
    if __DEBUG_ON:
        print(f"   - now : {datetime.datetime.fromtimestamp(now).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   - __last_setup_time : {datetime.datetime.fromtimestamp(__last_setup_time).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   - __is_setup_print_log : {__is_setup_print_log}")
    
    
    # matplotlib ê²½ê³  ì–µì œ
    warnings.filterwarnings(action='ignore')
    
    # print("ğŸš€ Jupyter/Colab í•œê¸€ í™˜ê²½ ì„¤ì • ì¤‘... (helper v" + __version__ + ")")
    
    # NumPy í˜¸í™˜ì„± ì²´í¬
    _check_numpy_compatibility()
    
    try:
        
        if not _in_colab():
            os.system('chcp 65001')
            os.environ['PYTHONIOENCODING'] = 'utf-8'

        # í°íŠ¸ ë‹¤ìš´ë¡œë“œ/ì„¤ì¹˜ ë° ë¡œë”© (ì¶œë ¥ ìµœì†Œí™”)
        font_download_success = font_download()
        if font_download_success:
            font_load_success = load_font()
            if font_load_success:
                # pandas í™•ì¥ ê¸°ëŠ¥ ì„¤ì •
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    set_pandas_extension()
                
                if __is_setup_print_log:
                    print("âœ… ì„¤ì • ì™„ë£Œ: í•œê¸€ í°íŠ¸, plt ì „ì—­ ë“±ë¡, pandas í™•ì¥, ìºì‹œ ê¸°ëŠ¥")
                return
        print("âŒ ì„¤ì • ì‹¤íŒ¨")
    except Exception as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {str(e)}")


# =============================================================================
# PANDAS COMMIT SYSTEM
# =============================================================================

# pandas commit ì‹œìŠ¤í…œ DataFrame ë©”ì†Œë“œ wrappers

def _df_commit(self, msg, commit_dir=None):
    """
    DataFrameì˜ í˜„ì¬ ìƒíƒœë¥¼ ì»¤ë°‹í•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•:
        df.commit("ì»¤ë°‹ ë©”ì‹œì§€")
    """
    return pd_commit(self, msg, commit_dir)



@classmethod
def _df_checkout(cls, idx_or_hash, commit_dir=None):
    """
    DataFrame ì»¤ë°‹ ê¸°ë¡ì—ì„œ íŠ¹ì • ì»¤ë°‹ì„ ì²´í¬ì•„ì›ƒí•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•:
        pd.DataFrame.checkout(0)
    """
    return pd_checkout(idx_or_hash, commit_dir)

@classmethod
def _df_commit_list(cls, commit_dir=None):
    """
    DataFrameì˜ ì»¤ë°‹ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•:
        pd.DataFrame.commit_list()
    """
    return pd_commit_list(commit_dir)

@classmethod
def _df_commit_rm(cls, idx_or_hash, commit_dir=None):
    """
    DataFrame ì»¤ë°‹ ê¸°ë¡ì—ì„œ íŠ¹ì • ì»¤ë°‹ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•:
        pd.DataFrame.commit_rm(0)
    """
    return pd_commit_rm(idx_or_hash, commit_dir)

@classmethod
def _df_commit_has(cls, idx_or_hash, commit_dir=None):
    """
    DataFrame ì»¤ë°‹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•:
        pd.DataFrame.commit_has("ë©”ì‹œì§€")
    """
    return pd_commit_has(idx_or_hash, commit_dir)


# =============================================================================
# CACHE SYSTEM API
# =============================================================================

# ìºì‹œ ê´€ë ¨ helper API í•¨ìˆ˜ë“¤
def cache_key(*datas, **kwargs):
    """
    ì—¬ëŸ¬ ë°ì´í„°ì™€ í‚¤ì›Œë“œ ì¸ìë¥¼ ë°›ì•„ì„œ ê³ ìœ í•œ í•´ì‹œí‚¤ ìƒì„±
    
    Parameters:
    -----------
    *datas : any
        í•´ì‹œí‚¤ ìƒì„±ì— ì‚¬ìš©í•  ë°ì´í„°ë“¤
    **kwargs : any
        í•´ì‹œí‚¤ ìƒì„±ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ ì¸ìë“¤
    
    Returns:
    --------
    str : MD5 í•´ì‹œ í‚¤
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> key = helper.cache_key("model_v1", params)
    >>> print(key)  # '1a2b3c4d5e...'
    """
    return DataCatch.key(*datas, **kwargs)

def cache_save(key, value, cache_file=None):
    """
    ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥
    
    Parameters:
    -----------
    key : str
        ì €ì¥í•  ë•Œ ì‚¬ìš©í•  í‚¤
    value : any
        ì €ì¥í•  ë°ì´í„° (DataFrame, numpy array, ì¼ë°˜ ê°ì²´ ë“±)
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ 
        - None (ê¸°ë³¸ê°’): í™˜ê²½ë³„ ìë™ ì„¤ì •
          * Colab: /content/drive/MyDrive/cache.json
          * ë¡œì»¬: cache.json
        - ìƒëŒ€ ê²½ë¡œ: Colabì—ì„œ /content/drive/MyDrive/ í•˜ìœ„ì— ìë™ ì €ì¥
        - ì ˆëŒ€ ê²½ë¡œ: ì§€ì •ëœ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    Returns:
    --------
    bool : ì €ì¥ ì„±ê³µ ì—¬ë¶€
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> model = train_model()
    >>> key = helper.cache_key("model_v1", params)
    >>> helper.cache_save(key, model)  # í™˜ê²½ë³„ ê¸°ë³¸ ê²½ë¡œ
    >>> helper.cache_save(key, model, "project_a.json")  # Colab: /content/drive/MyDrive/project_a.json
    """
    return DataCatch.save(key, value, cache_file)

def cache_load(key, cache_file=None):
    """
    ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ
    
    Parameters:
    -----------
    key : str
        ë¡œë“œí•  ë°ì´í„°ì˜ í‚¤
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ
        - None (ê¸°ë³¸ê°’): í™˜ê²½ë³„ ìë™ ì„¤ì •
          * Colab: /content/drive/MyDrive/cache.json
          * ë¡œì»¬: cache.json
        - ìƒëŒ€ ê²½ë¡œ: Colabì—ì„œ /content/drive/MyDrive/ í•˜ìœ„ì—ì„œ ìë™ íƒìƒ‰
        - ì ˆëŒ€ ê²½ë¡œ: ì§€ì •ëœ ê²½ë¡œì—ì„œ ë¡œë“œ
    
    Returns:
    --------
    any or None : ì €ì¥ëœ ë°ì´í„° ë˜ëŠ” None (í‚¤ê°€ ì—†ì„ ê²½ìš°)
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> key = helper.cache_key("model_v1", params)
    >>> model = helper.cache_load(key)  # í™˜ê²½ë³„ ê¸°ë³¸ ê²½ë¡œì—ì„œ ë¡œë“œ
    >>> if model:
    >>>     print("ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œë¨")
    """
    return DataCatch.load(key, cache_file)

def cache_exists(key, cache_file=None):
    """
    ìºì‹œì— í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    
    Parameters:
    -----------
    key : str
        í™•ì¸í•  í‚¤
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Returns:
    --------
    bool : í‚¤ ì¡´ì¬ ì—¬ë¶€
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> key = helper.cache_key("model_v1", params)
    >>> if helper.cache_exists(key):
    >>>     model = helper.cache_load(key)
    """
    return DataCatch.exists(key, cache_file)

def cache_delete(key, cache_file=None):
    """
    ìºì‹œì—ì„œ íŠ¹ì • í‚¤ ì‚­ì œ
    
    Parameters:
    -----------
    key : str
        ì‚­ì œí•  í‚¤
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Returns:
    --------
    bool : ì‚­ì œ ì„±ê³µ ì—¬ë¶€
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_delete("old_model_key")
    """
    return DataCatch.delete(key, cache_file)

def cache_delete_keys(*keys, cache_file=None):
    """
    ìºì‹œì—ì„œ ì—¬ëŸ¬ í‚¤ë¥¼ í•œë²ˆì— ì‚­ì œ
    
    Parameters:
    -----------
    *keys : str
        ì‚­ì œí•  í‚¤ë“¤
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Returns:
    --------
    int : ì‚­ì œëœ í‚¤ì˜ ê°œìˆ˜
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_delete_keys("key1", "key2", "key3")
    """
    return DataCatch.delete_keys(*keys, cache_file=cache_file)

def cache_clear(cache_file=None):
    """
    ìºì‹œ ì „ì²´ ì´ˆê¸°í™”
    
    Parameters:
    -----------
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_clear()  # ëª¨ë“  ìºì‹œ ì‚­ì œ
    """
    DataCatch.clear_cache(cache_file)
    print("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

def cache_info(cache_file=None):
    """
    ìºì‹œ ì •ë³´ ì¶œë ¥
    
    Parameters:
    -----------
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_info()
    """
    DataCatch.cache_info(cache_file)

def cache_list_keys(cache_file=None):
    """
    ì €ì¥ëœ ëª¨ë“  í‚¤ ëª©ë¡ ë°˜í™˜
    
    Parameters:
    -----------
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Returns:
    --------
    list : í‚¤ ëª©ë¡
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> keys = helper.cache_list_keys()
    >>> print(f"ì €ì¥ëœ í‚¤ ê°œìˆ˜: {len(keys)}")
    """
    return DataCatch.list_keys(cache_file)

def cache_compress(cache_file=None):
    """
    ìºì‹œ íŒŒì¼ì„ ì••ì¶•í•˜ì—¬ ì €ì¥ ê³µê°„ ì ˆì•½
    
    Parameters:
    -----------
    cache_file : str, optional
        ì••ì¶•í•  ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_compress()  # ìºì‹œ íŒŒì¼ ì••ì¶•
    """
    return DataCatch.compress_cache(cache_file)

def cache_cleanup(days=30, cache_file=None):
    """
    ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì •ë¦¬ (í˜„ì¬ëŠ” ìˆ˜ë™ ì •ë¦¬ë§Œ ì§€ì›)
    
    Parameters:
    -----------
    days : int
        ë³´ê´€í•  ì¼ìˆ˜ (í˜„ì¬ ë¯¸êµ¬í˜„, í–¥í›„ í™•ì¥ìš©)
    cache_file : str, optional
        ì •ë¦¬í•  ìºì‹œ íŒŒì¼ ê²½ë¡œ
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> helper.cache_cleanup()  # ìˆ˜ë™ ì •ë¦¬
    """
    return DataCatch.cleanup_cache(days, cache_file)

def cache_size(cache_file=None):
    """
    ìºì‹œ í¬ê¸°(í•­ëª© ìˆ˜) ë°˜í™˜
    
    Parameters:
    -----------
    cache_file : str, optional
        ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: cache.json)
    
    Returns:
    --------
    int : ìºì‹œì— ì €ì¥ëœ í•­ëª© ìˆ˜
    
    Examples:
    ---------
    >>> import helper.c0z0c.dev as helper
    >>> size = helper.cache_size()
    >>> print(f"ìºì‹œ í¬ê¸°: {size}ê°œ")
    """
    return DataCatch.size(cache_file)


# =============================================================================
# PANDAS EXTENSION: BASIC COLUMN DESCRIPTION FUNCTIONS
# =============================================================================

def set_head_att(self, key_or_dict, value=None):
    """
    ì»¬ëŸ¼ ì„¤ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    key_or_dict : dict or str
        - dict: ì—¬ëŸ¬ ì»¬ëŸ¼ ì„¤ëª…ì„ í•œ ë²ˆì— ì„¤ì • {"ì»¬ëŸ¼ëª…": "ì„¤ëª…"}
        - str: ë‹¨ì¼ ì»¬ëŸ¼ëª… (valueì™€ í•¨ê»˜ ì‚¬ìš©)
    value : str, optional
        key_or_dictê°€ strì¼ ë•Œ í•´ë‹¹ ì»¬ëŸ¼ì˜ ì„¤ëª…
    
    Examples:
    ---------
    >>> df.set_head_att({"id": "ID", "state": "ì§€ì—­"})
    >>> df.set_head_att("id", "ì•„ì´ë””")
    """
    # attrs ì´ˆê¸°í™”
    if not hasattr(self, 'attrs'):
        self.attrs = {}
    if 'column_descriptions' not in self.attrs:
        self.attrs["column_descriptions"] = {}
    
    if isinstance(key_or_dict, dict):
        # ë”•ì…”ë„ˆë¦¬ë¡œ ì—¬ëŸ¬ ê°œ ì„¤ì •
        self.attrs["column_descriptions"].update(key_or_dict)
    elif isinstance(key_or_dict, str) and value is not None:
        # ê°œë³„ ì„¤ì •/ìˆ˜ì •
        self.attrs["column_descriptions"][key_or_dict] = value
    else:
        raise ValueError("ì‚¬ìš©ë²•: set_head_att(dict) ë˜ëŠ” set_head_att(key, value)")

def get_head_att(self, key=None):
    """
    ì»¬ëŸ¼ ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    key : str, optional
        íŠ¹ì • ì»¬ëŸ¼ì˜ ì„¤ëª…ì„ ê°€ì ¸ì˜¬ ì»¬ëŸ¼ëª…. Noneì´ë©´ ì „ì²´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    
    Returns:
    --------
    dict or str : 
        - keyê°€ Noneì´ë©´ ì „ì²´ ì»¬ëŸ¼ ì„¤ëª… ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        - keyê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì»¬ëŸ¼ì˜ ì„¤ëª… ë¬¸ìì—´ ë°˜í™˜
    
    Raises:
    -------
    KeyError : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ëª…ì„ ìš”ì²­í–ˆì„ ë•Œ
    TypeError : keyê°€ ë¬¸ìì—´ì´ ì•„ë‹ ë•Œ
    
    Examples:
    ---------
    >>> descriptions = df.get_head_att()           # ì „ì²´ ë”•ì…”ë„ˆë¦¬
    >>> score_desc = df.get_head_att('score')     # íŠ¹ì • ì»¬ëŸ¼ ì„¤ëª…
    >>> descriptions['new_col'] = 'ìƒˆë¡œìš´ ì„¤ëª…'    # ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ìˆ˜ì • ê°€ëŠ¥
    """
    # attrs ì´ˆê¸°í™”
    if not hasattr(self, 'attrs'):
        self.attrs = {}
    if 'column_descriptions' not in self.attrs:
        self.attrs["column_descriptions"] = {}
    
    # keyê°€ Noneì´ë©´ ì „ì²´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    if key is None:
        return self.attrs["column_descriptions"]
    
    # key íƒ€ì… ê²€ì¦
    if not isinstance(key, str):
        raise TypeError(f"keyëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(key)}")
    
    # key ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if key not in self.attrs["column_descriptions"]:
        return key  # ì»¬ëŸ¼ ì„¤ëª…ì´ ì—†ìœ¼ë©´ key ìì²´ ë°˜í™˜ (None ëŒ€ì‹ )
        #available_keys = list(self.attrs["column_descriptions"].keys())
        #raise KeyError(f"ì»¬ëŸ¼ '{key}'ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_keys}")
    
    return self.attrs["column_descriptions"][key]

def remove_head_att(self, key):
    """
    íŠ¹ì • ì»¬ëŸ¼ ì„¤ëª… ë˜ëŠ” ì»¬ëŸ¼ ì„¤ëª… ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
    
    Parameters:
    -----------
    key : str or list
        ì‚­ì œí•  ì»¬ëŸ¼ëª… ë˜ëŠ” ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
    """
    if not hasattr(self, 'attrs') or 'column_descriptions' not in self.attrs:
        return

    if isinstance(key, str):
        key = [key]

    for k in key:
        if k in self.attrs["column_descriptions"]:
            self.attrs["column_descriptions"].pop(k)
            print(f"ì»¬ëŸ¼ ì„¤ëª… '{k}' ì‚­ì œ ì™„ë£Œ")
        else:
            print(f"'{k}' ì»¬ëŸ¼ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def clear_head_att(self):
    """ëª¨ë“  ì»¬ëŸ¼ ì„¤ëª…ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if not hasattr(self, 'attrs'):
        self.attrs = {}
    self.attrs["column_descriptions"] = {}

def _align_text(text, width, align='left'):
    """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í­ì— ë§ì¶° ì •ë ¬"""
    text_str = str(text)
    current_width = _get_text_width(text_str)
    padding = max(0, width - current_width)
    
    if align == 'right':
        return ' ' * padding + text_str
    elif align == 'center':
        left_padding = padding // 2
        right_padding = padding - left_padding
        return ' ' * left_padding + text_str + ' ' * right_padding
    else:  # left (default)
        return text_str + ' ' * padding

def _calculate_column_widths(df_display, labels):
    """ì»¬ëŸ¼ í­ ê³„ì‚° (pandas ê¸°ë³¸ ìŠ¤íƒ€ì¼)"""
    widths = []
    
    # ì²« ë²ˆì§¸ ì»¬ëŸ¼: ì¸ë±ìŠ¤ í­ ê³„ì‚°
    if len(df_display) == 0:
        max_index_width = 1  # ìµœì†Œ í­
    else:
        max_index_width = max(_get_text_width(str(idx)) for idx in df_display.index)
    
    # ì¸ë±ìŠ¤ ì»¬ëŸ¼ í­ (pandas ìŠ¤íƒ€ì¼: ìµœì†Œ ì—¬ìœ  ê³µê°„)
    index_width = max_index_width + 1
    widths.append(index_width)
    
    # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
    for col in df_display.columns:
        korean_name = labels.get(col, col)
        english_name = col
        
        # ë°ì´í„°ê°€ ë¹„ì–´ìˆì„ ë•Œ ì²˜ë¦¬
        if len(df_display) == 0:
            max_data_width = 0
        else:
            max_data_width = max(_get_text_width(_format_value(val)) for val in df_display[col])
        
        # ê° ìš”ì†Œì˜ ìµœëŒ€ í­ ê³„ì‚°
        max_width = max(
            _get_text_width(korean_name),
            _get_text_width(english_name),
            max_data_width
        )
        
        # pandas ìŠ¤íƒ€ì¼: ìµœì†Œ ì—¬ìœ  ê³µê°„ (1ì¹¸)
        column_width = max_width + 1
        widths.append(column_width)
    
    return widths

def pd_head_att(self, rows=5, out=None):
    """í•œê¸€ ì»¬ëŸ¼ ì„¤ëª…ì´ í¬í•¨ëœ DataFrameì„ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    import pandas as pd
    df.head_att()
    df.head_att(rows=5, out='print')
    df.head_att(rows='all', out='html')
    Parameters:
    -----------
    rows : int or str, optional
        ì¶œë ¥í•  í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    out : str, optional
        ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: 'print')
        'print', 'html', 'str' ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Returns:
    --------
    str or None
        - 'print'ì¼ ê²½ìš° None ë°˜í™˜ (ì½˜ì†” ì¶œë ¥)
        - 'html'ì¼ ê²½ìš° HTML ê°ì²´ ë°˜í™˜
        - 'str'ì¼ ê²½ìš° ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜
    Raises:
    -------
    ValueError : ì˜ëª»ëœ out ì˜µì…˜
    Examples:
    ---------
    >>> df.head_att()  # ê¸°ë³¸ ì¶œë ¥ (5í–‰)
    >>> df.head_att(rows=10)  # 10í–‰ ì¶œë ¥
    >>> df.head_att(out='html')  # HTML í˜•íƒœë¡œ ì¶œë ¥
    >>> df.head_att(rows='all', out='print')  # ì „ì²´ ë°ì´í„° ì¶œë ¥ (ì½˜ì†”)
    """
    labels = self.attrs.get("column_descriptions", {})

    # ì¶œë ¥í•  ë°ì´í„° ê²°ì •
    if isinstance(rows, str) and rows.lower() == "all":
        df_display = self
    elif isinstance(rows, int):
        if rows == -1:
            df_display = self
        elif rows == 0:
            df_display = self.iloc[0:0]
        else:
            df_display = self.head(rows)
    else:
        df_display = self.head(5)

    # ë³´ì¡° ì»¬ëŸ¼ëª… ì¶œë ¥ ì¡°ê±´
    # 1. column_descriptionsê°€ ì™„ì „íˆ ë¹„ì–´ ìˆìœ¼ë©´ ë³´ì¡° ì»¬ëŸ¼ëª… ì¶œë ¥í•˜ì§€ ì•ŠìŒ (ì˜¤ë¦¬ì§€ë„ ì»¬ëŸ¼ëª…ë§Œ í•œ ë²ˆ ì¶œë ¥)
    # 2. column_descriptionsê°€ ë¹„ì–´ ìˆì§€ ì•Šê³  íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    if not labels:
        # ë³´ì¡° ì»¬ëŸ¼ëª… ì—†ì´ ì˜¤ë¦¬ì§€ë„ ì»¬ëŸ¼ëª…ë§Œ í•œ ë²ˆ ì¶œë ¥
        def _print_original_only(df_display):
            # ì˜ë¬¸ í—¤ë” ì¶œë ¥ (ì˜¤ë¥¸ìª½ ì •ë ¬)
            column_widths = _calculate_column_widths(df_display, {})
            index_width = column_widths[0]
            data_widths = column_widths[1:]
            english_parts = []
            english_parts.append(_align_text('', index_width, 'right'))
            for col, width in zip(df_display.columns, data_widths):
                english_parts.append(_align_text(col, width, 'right'))
            print(''.join(english_parts))
            # ë°ì´í„° ì¶œë ¥
            for idx, row in df_display.iterrows():
                row_parts = []
                row_parts.append(_align_text(str(idx), index_width, 'right'))
                for val, width in zip(row, data_widths):
                    row_parts.append(_align_text(_format_value(val), width, 'right'))
                print(''.join(row_parts))
        if out is None or out.lower() == 'print':
            _print_original_only(df_display)
            return None
        elif out.lower() == 'html':
            # HTML í—¤ë”ëŠ” ì˜¤ë¦¬ì§€ë„ ì»¬ëŸ¼ëª…ë§Œ ì¶œë ¥
            df_copy = df_display.copy()
            # ì‹¤ìˆ˜í˜• ê°’ë“¤ì„ í¬ë§·íŒ…
            for col in df_copy.columns:
                df_copy[col] = df_copy[col].apply(_format_value)
            df_copy.columns = list(df_display.columns)
            if IPYTHON_AVAILABLE:
                return HTML(df_copy.to_html(escape=False))
            else:
                return df_copy.to_html(escape=False)
        elif out.lower() in ['str', 'string']:
            # ë¬¸ìì—´ í˜•íƒœë¡œ ì˜¤ë¦¬ì§€ë„ ì»¬ëŸ¼ëª…ë§Œ ì¶œë ¥
            column_widths = _calculate_column_widths(df_display, {})
            result = ""
            english_row = ""
            for i, col in enumerate(df_display.columns):
                english_row += _align_text(col, column_widths[i])
            result += english_row + "\n"
            for idx, row in df_display.iterrows():
                data_row = ""
                for i, val in enumerate(row):
                    if i == 0:
                        text = str(idx)
                        formatted_val = _format_value(val)
                        data_row += _align_text(text, column_widths[i] - _get_text_width(formatted_val))
                        data_row += formatted_val
                    else:
                        data_row += _align_text(_format_value(val), column_widths[i])
                result += data_row + "\n"
            return result.rstrip()
        else:
            raise ValueError("out ì˜µì…˜ì€ 'html', 'print', 'str', 'string' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    else:
        # ê¸°ì¡´ ë¡œì§ (ë³´ì¡° ì»¬ëŸ¼ëª… ì¼ë¶€ë§Œ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
        if out is None or out.lower() == 'print':
            return self._print_head_att(df_display, labels)
        elif out.lower() == 'html':
            return self._html_head_att(df_display, labels)
        elif out.lower() in ['str', 'string']:
            return self._string_head_att(df_display, labels)
        else:
            raise ValueError("out ì˜µì…˜ì€ 'html', 'print', 'str', 'string' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

def _print_head_att(self, df_display, labels):
    """print í˜•íƒœë¡œ ì¶œë ¥ (pandas ê¸°ë³¸ ìŠ¤íƒ€ì¼)"""
    column_widths = _calculate_column_widths(df_display, labels)
    
    # ì²« ë²ˆì§¸ ë¶€ë¶„ì€ ì¸ë±ìŠ¤ìš©
    index_width = column_widths[0]
    data_widths = column_widths[1:]
    
    # í•œê¸€ í—¤ë” ì¶œë ¥ (ì˜¤ë¥¸ìª½ ì •ë ¬)
    korean_parts = []
    korean_parts.append(_align_text('', index_width, 'right'))  # ì¸ë±ìŠ¤ í—¤ë”ëŠ” ë¹ˆê³µê°„
    for col, width in zip(df_display.columns, data_widths):
        korean_name = labels.get(col, col)
        korean_parts.append(_align_text(korean_name, width, 'right'))
    print(''.join(korean_parts))
    
    # ì˜ë¬¸ í—¤ë” ì¶œë ¥ (ì˜¤ë¥¸ìª½ ì •ë ¬)
    english_parts = []
    english_parts.append(_align_text('', index_width, 'right'))  # ì¸ë±ìŠ¤ í—¤ë”ëŠ” ë¹ˆê³µê°„
    for col, width in zip(df_display.columns, data_widths):
        english_parts.append(_align_text(col, width, 'right'))
    print(''.join(english_parts))
    
    # ë°ì´í„° ì¶œë ¥ (ëª¨ë‘ ì˜¤ë¥¸ìª½ ì •ë ¬ - pandas ê¸°ë³¸ ìŠ¤íƒ€ì¼)
    for idx, row in df_display.iterrows():
        row_parts = []
        # ì¸ë±ìŠ¤ ì¶œë ¥ (ì˜¤ë¥¸ìª½ ì •ë ¬)
        row_parts.append(_align_text(str(idx), index_width, 'right'))
        # ë°ì´í„° ì¶œë ¥ (ì˜¤ë¥¸ìª½ ì •ë ¬)
        for val, width in zip(row, data_widths):
            row_parts.append(_align_text(_format_value(val), width, 'right'))
        print(''.join(row_parts))

def _html_head_att(self, df_display, labels):
    """HTML í˜•íƒœë¡œ ì¶œë ¥"""
    header = []
    for col in df_display.columns:
        if col in labels and labels[col]:
            header.append(f"{labels[col]}<br>{col}")
        else:
            header.append(col)
    
    df_copy = df_display.copy()
    # ì‹¤ìˆ˜í˜• ê°’ë“¤ì„ í¬ë§·íŒ…
    for col in df_copy.columns:
        df_copy[col] = df_copy[col].apply(_format_value)
    df_copy.columns = header
    
    if IPYTHON_AVAILABLE:
        return HTML(df_copy.to_html(escape=False))
    else:
        return df_copy.to_html(escape=False)

def _string_head_att(self, df_display, labels):
    """ë¬¸ìì—´ í˜•íƒœë¡œ ì¶œë ¥"""
    column_widths = _calculate_column_widths(df_display, labels)
    
    result = ""
    
    # í•œê¸€ í—¤ë” ìƒì„±
    korean_row = ""
    for i, col in enumerate(df_display.columns):
        korean_name = labels.get(col, col)
        korean_row += _align_text(korean_name, column_widths[i])
    result += korean_row + "\n"
    
    # ì˜ë¬¸ í—¤ë” ìƒì„±
    english_row = ""
    for i, col in enumerate(df_display.columns):
        english_row += _align_text(col, column_widths[i])
    result += english_row + "\n"
    
    # ë°ì´í„° ìƒì„±
    for idx, row in df_display.iterrows():
        data_row = ""
        for i, val in enumerate(row):
            if i == 0:
                text = str(idx)
                formatted_val = _format_value(val)
                data_row += _align_text(text, column_widths[i] - _get_text_width(formatted_val))
                data_row += formatted_val
            else:
                data_row += _align_text(_format_value(val), column_widths[i])
        result += data_row + "\n"
    
    return result.rstrip()

def series_head_att(self, rows=5, out=None):
    """í•œê¸€ ì»¬ëŸ¼ ì„¤ëª…ì´ í¬í•¨ëœ Seriesë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    labels = self.attrs.get("column_descriptions", {})
    
    # ì¶œë ¥í•  ë°ì´í„° ê²°ì •
    if isinstance(rows, str) and rows.lower() == "all":
        series_display = self
    elif isinstance(rows, int):
        if rows == -1:
            series_display = self
        elif rows == 0:
            series_display = self.iloc[0:0]
        else:
            series_display = self.head(rows)
    else:
        series_display = self.head(5)
    
    series_name = self.name if self.name is not None else "Series"
    korean_name = labels.get(series_name, series_name)
    
    if out is None or out.lower() == 'print':
        # ì¸ë±ìŠ¤ ìµœëŒ€ í­ ê³„ì‚°
        index_widths = [_get_text_width(str(idx)) for idx in series_display.index]
        max_index_width = max(index_widths) if index_widths else 0
        
        # ë°ì´í„° ìµœëŒ€ í­ ê³„ì‚°
        data_widths = [_get_text_width(_format_value(val)) for val in series_display]
        max_data_width = max(data_widths) if data_widths else 0
        
        # í—¤ë” í­ ê³„ì‚°
        korean_header_width = _get_text_width(korean_name)
        english_header_width = _get_text_width(series_name)
        
        # ê° ì»¬ëŸ¼ì˜ ìµœëŒ€ í­ ê²°ì •
        index_column_width = max(max_index_width, 5) + 2
        data_column_width = max(max_data_width, korean_header_width, english_header_width) + 2
        
        # í—¤ë” ì¶œë ¥
        korean_header = _align_text("ì¸ë±ìŠ¤", index_column_width) + _align_text(korean_name, data_column_width)
        print(korean_header)
        
        english_header = _align_text("index", index_column_width) + _align_text(series_name, data_column_width)
        print(english_header)
        
        # ë°ì´í„° ì¶œë ¥
        for idx, val in series_display.items():
            data_row = _align_text(str(idx), index_column_width) + _align_text(_format_value(val), data_column_width)
            print(data_row)
        
        return None
    
    elif out.lower() == 'html':
        df = series_display.to_frame()
        # ì‹¤ìˆ˜í˜• ê°’ë“¤ì„ í¬ë§·íŒ…
        df.iloc[:, 0] = df.iloc[:, 0].apply(_format_value)
        
        if series_name in labels and labels[series_name]:
            df.columns = [f"{labels[series_name]}<br>{series_name}"]
        else:
            df.columns = [series_name]
        
        if IPYTHON_AVAILABLE:
            return HTML(df.to_html(escape=False))
        else:
            return df.to_html(escape=False)
    
    elif out.lower() in ['str', 'string']:
        # ì¸ë±ìŠ¤ ìµœëŒ€ í­ ê³„ì‚°
        index_widths = [_get_text_width(str(idx)) for idx in series_display.index]
        max_index_width = max(index_widths) if index_widths else 0
        
        # ë°ì´í„° ìµœëŒ€ í­ ê³„ì‚°
        data_widths = [_get_text_width(_format_value(val)) for val in series_display]
        max_data_width = max(data_widths) if data_widths else 0
        
        # í—¤ë” í­ ê³„ì‚°
        korean_header_width = _get_text_width(korean_name)
        english_header_width = _get_text_width(series_name)
        
        # ê° ì»¬ëŸ¼ì˜ ìµœëŒ€ í­ ê²°ì •
        index_column_width = max(max_index_width, _get_text_width("ì¸ë±ìŠ¤"), _get_text_width("index")) + 2
        data_column_width = max(max_data_width, korean_header_width, english_header_width) + 2
        
        result = ""
        
        # í•œê¸€ í—¤ë” ìƒì„±
        korean_header = _align_text("ì¸ë±ìŠ¤", index_column_width) + _align_text(korean_name, data_column_width)
        result += korean_header + "\n"
        
        # ì˜ë¬¸ í—¤ë” ìƒì„±
        english_header = _align_text("index", index_column_width) + _align_text(series_name, data_column_width)
        result += english_header + "\n"
        
        # ë°ì´í„° ìƒì„±
        for idx, val in series_display.items():
            data_row = _align_text(str(idx), index_column_width) + _align_text(_format_value(val), data_column_width)
            result += data_row + "\n"
        
        return result.rstrip()
    
    else:
        raise ValueError("out ì˜µì…˜ì€ 'html', 'print', 'str', 'string' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")


# =============================================================================
# PANDAS EXTENSION: COLUMN SET MANAGEMENT FUNCTIONS
# =============================================================================

def _init_column_attrs(self):
    """ì»¬ëŸ¼ ì†ì„± ì´ˆê¸°í™”"""
    if not hasattr(self, 'attrs'):
        self.attrs = {}
    if 'columns_extra' not in self.attrs:
        self.attrs['columns_extra'] = {
            'org': {'name': 'org', 'columns': {col: col for col in self.columns}}
        }
        self.attrs['current_column_set'] = 'org'

def set_head_ext(self, columns_name, columns_extra=None, column_value=None):
    """
    ë³´ì¡° ì»¬ëŸ¼ëª… ì„¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    
    ì‚¬ìš©ë²•:
    1. ì „ì²´ ì„¸íŠ¸ ì„¤ì •: set_head_ext('kr', {'id': 'ID', 'name': 'ì´ë¦„'})
    2. ê°œë³„ ì»¬ëŸ¼ ì„¤ì •: set_head_ext('kr', 'name', 'ì´ë¦„')
    
    Parameters:
    -----------
    columns_name : str
        ì»¬ëŸ¼ ì„¸íŠ¸ì˜ ì´ë¦„ (ì˜ˆ: 'kr', 'desc', 'eng')
    columns_extra : dict or str
        ë°©ì‹1: ì „ì²´ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ {"ì›ë³¸ì»¬ëŸ¼": "ìƒˆì»¬ëŸ¼ëª…"}
        ë°©ì‹2: ê°œë³„ ì»¬ëŸ¼ëª… (í‚¤)
    column_value : str, optional
        ë°©ì‹2ì—ì„œ ì‚¬ìš©í•  ì»¬ëŸ¼ ê°’
    
    Raises:
    -------
    TypeError : ì˜ëª»ëœ íƒ€ì…ì˜ ë§¤ê°œë³€ìˆ˜
    ValueError : ì˜ëª»ëœ ê°’ (ë¹ˆ ë¬¸ìì—´, ë¹ˆ ë”•ì…”ë„ˆë¦¬, None ê°’, ì¤‘ë³µê°’ ë“±)
    KeyError : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ëª…
    
    Examples:
    ---------
    >>> df.set_head_ext('kr', {'id': 'ID', 'name': 'ì´ë¦„'})
    >>> df.set_head_ext('kr', 'score', 'ì ìˆ˜')  # ê°œë³„ ì¶”ê°€
    >>> df.set_head_ext('desc', {'id': 'ì‹ë³„ì', 'name': 'ì„±ëª…'})
    """
    # ì…ë ¥ ë°©ì‹ íŒë‹¨
    if column_value is not None:
        # ë°©ì‹ 2: ê°œë³„ ì»¬ëŸ¼ ì„¤ì •
        return self._set_head_ext_individual(columns_name, columns_extra, column_value)
    else:
        # ë°©ì‹ 1: ì „ì²´ ì„¸íŠ¸ ì„¤ì •
        return self._set_head_ext_bulk(columns_name, columns_extra)

def _set_head_ext_bulk(self, columns_name, columns_extra):
    """ì „ì²´ ì„¸íŠ¸ ì„¤ì • (ê¸°ì¡´ ë°©ì‹)"""
    # 1. ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(columns_name, str):
        raise TypeError(f"columns_nameì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(columns_name)}")
    
    if not isinstance(columns_extra, dict):
        raise TypeError(f"columns_extraëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(columns_extra)}")
    
    # 2. ë¹ˆ ì´ë¦„ ê²€ì¦
    if not columns_name.strip():
        raise ValueError("columns_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 3. ë¹ˆ ë”•ì…”ë„ˆë¦¬ ê²€ì¦
    if not columns_extra:
        raise ValueError("columns_extraëŠ” ìµœì†Œ í•˜ë‚˜ì˜ ì»¬ëŸ¼ ë§¤í•‘ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # 4. í˜„ì¬ DataFrameì˜ ì»¬ëŸ¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    current_columns = set(self.columns)
    
    # 5. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ê²€ì¦
    missing_columns = set(columns_extra.keys()) - current_columns
    if missing_columns:
        raise KeyError(f"ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì´ DataFrameì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {list(missing_columns)}")
    
    # 6. None ê°’ ê²€ì¦
    none_mappings = [k for k, v in columns_extra.items() if v is None]
    if none_mappings:
        raise ValueError(f"ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì˜ ë§¤í•‘ ê°’ì´ Noneì…ë‹ˆë‹¤: {none_mappings}")
    
    # 7. ì¤‘ë³µëœ ìƒˆ ì»¬ëŸ¼ëª… ê²€ì¦
    new_column_names = list(columns_extra.values())
    duplicates = [name for name in new_column_names if new_column_names.count(name) > 1]
    if duplicates:
        unique_duplicates = list(set(duplicates))
        raise ValueError(f"ì¤‘ë³µëœ ìƒˆ ì»¬ëŸ¼ëª…ì´ ìˆìŠµë‹ˆë‹¤: {unique_duplicates}")
    
    # 8. ì˜ˆì•½ëœ ì„¸íŠ¸ëª… ê²€ì¦
    if columns_name == 'org':
        raise ValueError("'org'ëŠ” ì˜ˆì•½ëœ ì„¸íŠ¸ëª…ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # ëª¨ë“  ê²€ì¦ì„ í†µê³¼í•˜ë©´ ê¸°ì¡´ ë¡œì§ ì‹¤í–‰
    self._init_column_attrs()
    
    self.attrs['columns_extra'][columns_name] = {
        'name': columns_name,
        'columns': columns_extra.copy()
    }
    
    print(f"ì»¬ëŸ¼ ì„¸íŠ¸ '{columns_name}' ì„¤ì • ì™„ë£Œ ({len(columns_extra)}ê°œ)")

def _set_head_ext_individual(self, columns_name, column_key, column_value):
    """ê°œë³„ ì»¬ëŸ¼ ì„¤ì • (ìƒˆë¡œìš´ ë°©ì‹)"""
    # ì…ë ¥ ê²€ì¦
    if not isinstance(columns_name, str):
        raise TypeError(f"columns_nameì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(columns_name)}")
    
    if not isinstance(column_key, str):
        raise TypeError(f"column_keyëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(column_key)}")
    
    if column_value is None:
        raise ValueError("column_valueëŠ” Noneì¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not columns_name.strip():
        raise ValueError("columns_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if columns_name == 'org':
        raise ValueError("'org'ëŠ” ì˜ˆì•½ëœ ì„¸íŠ¸ëª…ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    if column_key not in self.columns:
        raise KeyError(f"ì»¬ëŸ¼ '{column_key}'ì´ DataFrameì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    self._init_column_attrs()
    
    # ì„¸íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
    if columns_name not in self.attrs['columns_extra']:
        self.attrs['columns_extra'][columns_name] = {
            'name': columns_name,
            'columns': {}
        }
    
    # ê°œë³„ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
    old_value = self.attrs['columns_extra'][columns_name]['columns'].get(column_key)
    self.attrs['columns_extra'][columns_name]['columns'][column_key] = column_value
    
    if old_value is None:
        print(f"'{columns_name}': '{column_key}' â†’ '{column_value}' ì¶”ê°€")
    else:
        print(f"'{columns_name}': '{column_key}' ìˆ˜ì •ë¨")

def set_head_column(self, columns_name):
    """
    ì§€ì •ëœ ì»¬ëŸ¼ ì„¸íŠ¸ë¡œ DataFrameì˜ ì»¬ëŸ¼ëª…ì„ ë³€ê²½í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    columns_name : str
        ì‚¬ìš©í•  ì»¬ëŸ¼ ì„¸íŠ¸ ì´ë¦„ (ì˜ˆ: 'kr', 'desc', 'org')
    
    Raises:
    -------
    TypeError : ì˜ëª»ëœ íƒ€ì…ì˜ ë§¤ê°œë³€ìˆ˜
    ValueError : ì˜ëª»ëœ ê°’ (ë¹ˆ ë¬¸ìì—´ ë“±)
    KeyError : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì„¸íŠ¸ëª…
    
    Examples:
    ---------
    >>> df.set_head_column('kr')   # í•œê¸€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
    >>> df.set_head_column('org')  # ì›ë³¸ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³µì›
    """
    # 1. ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(columns_name, str):
        raise TypeError(f"columns_nameì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(columns_name)}")
    
    # 2. ë¹ˆ ë¬¸ìì—´ ê²€ì¦
    if not columns_name.strip():
        raise ValueError("columns_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    self._init_column_attrs()
    
    # 3. ì»¬ëŸ¼ ì„¸íŠ¸ ì¡´ì¬ ê²€ì¦
    if columns_name not in self.attrs['columns_extra']:
        available = list(self.attrs['columns_extra'].keys())
        raise KeyError(f"'{columns_name}' ì»¬ëŸ¼ ì„¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸íŠ¸: {available}")
    
    current_set = self.get_current_column_set()
    target_columns = self.attrs['columns_extra'][columns_name]['columns']
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ ë¡œì§
    new_columns = self._convert_columns(current_set, columns_name, target_columns)
    self.columns = new_columns
    self.attrs['current_column_set'] = columns_name
    
    self._update_column_descriptions(current_set, columns_name)
    
    print(f"ì»¬ëŸ¼ëª… ë³€ê²½: '{current_set}' â†’ '{columns_name}'")

def _convert_columns(self, current_set, target_set, target_columns):
    """ì»¬ëŸ¼ëª… ë³€í™˜ ë¡œì§"""
    current_columns = self.attrs['columns_extra'][current_set]['columns']
    current_to_org = {v: k for k, v in current_columns.items()}
    
    new_columns = []
    for current_col in self.columns:
        if current_col in current_to_org:
            org_col = current_to_org[current_col]
        else:
            org_col = current_col
        
        if org_col in target_columns:
            new_columns.append(target_columns[org_col])
        else:
            new_columns.append(org_col)
    
    return new_columns

def _update_column_descriptions(self, current_set, target_set):
    """ì»¬ëŸ¼ ì„¤ëª… ì—…ë°ì´íŠ¸"""
    if 'column_descriptions' not in self.attrs:
        return
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ ì „ì˜ old_columnsì™€ ë³€ê²½ í›„ì˜ new_columns(self.columns) ë§¤í•‘
    current_columns = self.attrs['columns_extra'][current_set]['columns']
    target_columns = self.attrs['columns_extra'][target_set]['columns']
    
    # í˜„ì¬ ì»¬ëŸ¼ëª… â†’ ì›ë³¸ ì»¬ëŸ¼ëª… ë§¤í•‘
    current_to_org = {v: k for k, v in current_columns.items()}
    
    # ë³€ê²½ ì „ ì»¬ëŸ¼ëª… ëª©ë¡ ìƒì„± (í˜„ì¬ self.columnsëŠ” ì´ë¯¸ ë³€ê²½ëœ ìƒíƒœ)
    old_columns = []
    for new_col in self.columns:  # new_colì€ ë³€ê²½ëœ ì»¬ëŸ¼ëª…
        # ìƒˆ ì»¬ëŸ¼ëª…ì—ì„œ ì›ë³¸ ì»¬ëŸ¼ëª… ì°¾ê¸°
        target_to_org = {v: k for k, v in target_columns.items()}
        if new_col in target_to_org:
            org_col = target_to_org[new_col]
            # ì›ë³¸ ì»¬ëŸ¼ëª…ì—ì„œ ì´ì „ ì»¬ëŸ¼ëª… ì°¾ê¸°
            if org_col in current_columns:
                old_columns.append(current_columns[org_col])
            else:
                old_columns.append(org_col)
        else:
            old_columns.append(new_col)
    
    old_descriptions = self.attrs['column_descriptions'].copy()
    new_descriptions = {}
    
    # ë³€ê²½ ì „ ì»¬ëŸ¼ëª…ê³¼ ë³€ê²½ í›„ ì»¬ëŸ¼ëª…ì„ ë§¤í•‘
    for old_col, new_col in zip(old_columns, self.columns):
        if old_col in old_descriptions:
            new_descriptions[new_col] = old_descriptions[old_col]
    
    self.attrs['column_descriptions'] = new_descriptions

def get_current_column_set(self):
    """
    í˜„ì¬ í™œì„±í™”ëœ ì»¬ëŸ¼ ì„¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
    --------
    str : í˜„ì¬ ì»¬ëŸ¼ ì„¸íŠ¸ ì´ë¦„
    """
    if not hasattr(self, 'attrs'):
        return 'org'
    return self.attrs.get('current_column_set', 'org')

def get_head_ext(self, columns_name=None):
    """
    ë³´ì¡° ì»¬ëŸ¼ëª… ì„¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    columns_name : str, optional
        íŠ¹ì • ì»¬ëŸ¼ ì„¸íŠ¸ ì´ë¦„. Noneì´ë©´ ì „ì²´ ë°˜í™˜
    
    Returns:
    --------
    dict : ì»¬ëŸ¼ ì„¸íŠ¸ ì •ë³´
    """
    if not hasattr(self, 'attrs'):
        self.attrs = {}
    if 'columns_extra' not in self.attrs:
        self.attrs['columns_extra'] = {}
    
    if columns_name is None:
        return self.attrs['columns_extra']
    else:
        return self.attrs['columns_extra'].get(columns_name, {})

def list_head_ext(self):
    """ë“±ë¡ëœ ëª¨ë“  ì»¬ëŸ¼ ì„¸íŠ¸ ì¶œë ¥"""
    self._init_column_attrs()
    
    if not self.attrs['columns_extra']:
        print(" ë“±ë¡ëœ ì»¬ëŸ¼ ì„¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    current_set = self.get_current_column_set()
    max_name_length = max(len(name) for name in self.attrs['columns_extra'].keys())
    
    print(" ë“±ë¡ëœ ì»¬ëŸ¼ ì„¸íŠ¸:")
    for name, info in self.attrs['columns_extra'].items():
        columns_list = list(info['columns'].values() if name != 'org' else info['columns'].keys())
        status = " (í˜„ì¬)" if name == current_set else ""
        formatted_name = f"{name}{status}".rjust(max_name_length + 5)
        print(f"{formatted_name}: {columns_list}")

def clear_head_ext(self):
    """ì»¬ëŸ¼ëª…ì„ ì›ë³¸ìœ¼ë¡œ ë³µì› ë° ì»¬ëŸ¼ ì„¸íŠ¸ ì´ˆê¸°í™”"""
    if not hasattr(self, 'attrs') or 'columns_extra' not in self.attrs:
        return
    
    if 'org' in self.attrs['columns_extra']:
        org_columns = list(self.attrs['columns_extra']['org']['columns'].keys())
        self.columns = org_columns
        self.attrs['current_column_set'] = 'org'
        print(" ì»¬ëŸ¼ëª…ì„ ì›ë³¸ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤.")
    
    # org ì œì™¸í•˜ê³  ëª¨ë“  ì»¬ëŸ¼ ì„¸íŠ¸ ì´ˆê¸°í™”
    org_backup = self.attrs['columns_extra'].get('org', {})
    self.attrs['columns_extra'] = {'org': org_backup}
    print(" ëª¨ë“  ì»¬ëŸ¼ ì„¸íŠ¸ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

def remove_head_ext(self, columns_name):
    """
    íŠ¹ì • ì»¬ëŸ¼ ì„¸íŠ¸ ë˜ëŠ” ì»¬ëŸ¼ ì„¸íŠ¸ ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
    Parameters:
    -----------
    columns_name : str or list
        ì‚­ì œí•  ì»¬ëŸ¼ ì„¸íŠ¸ëª… ë˜ëŠ” ì„¸íŠ¸ëª… ë¦¬ìŠ¤íŠ¸
    """
    if not hasattr(self, 'attrs') or 'columns_extra' not in self.attrs:
        return

    if isinstance(columns_name, str):
        columns_name = [columns_name]

    current_set = self.get_current_column_set()
    for name in columns_name:
        if name == 'org':
            print(" 'org' ì„¸íŠ¸ëŠ” ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        if name == current_set:
            print(f" í˜„ì¬ í™œì„±í™”ëœ '{name}' ì„¸íŠ¸ëŠ” ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(" ë¨¼ì € ë‹¤ë¥¸ ì„¸íŠ¸ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ì›ë³¸ìœ¼ë¡œ ë³µì›í•˜ì„¸ìš”.")
            continue
        if name in self.attrs['columns_extra']:
            del self.attrs['columns_extra'][name]
            print(f" ì»¬ëŸ¼ ì„¸íŠ¸ '{name}' ì‚­ì œ ì™„ë£Œ")
        else:
            print(f" '{name}' ì»¬ëŸ¼ ì„¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# =============================================================================
# CACHE SYSTEM CORE CLASS
# =============================================================================

class DataCatch:
    _default_cache_file = "cache.json"
    _cache = None
    _cache_file = None
    
    @classmethod
    def _initialize_cache(cls, cache_file=None):
        """ìºì‹œ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        if cls._cache is None:
            # ê¸°ë³¸ ìºì‹œ íŒŒì¼ ê²½ë¡œ ê²°ì •
            if cache_file is None:
                if _in_colab():
                    # Colab í™˜ê²½ì—ì„œëŠ” Google Drive ê²½ë¡œ ì‚¬ìš©
                    cls._cache_file = "/content/drive/MyDrive/cache.json"
                else:
                    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    cls._cache_file = cls._default_cache_file
            else:
                # ì‚¬ìš©ìê°€ ê²½ë¡œë¥¼ ì§€ì •í•œ ê²½ìš°
                if _in_colab() and not cache_file.startswith(('/', 'http://', 'https://')):
                    # Colabì—ì„œ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° Google Drive ê²½ë¡œë¡œ ë³€í™˜
                    cls._cache_file = f"/content/drive/MyDrive/{cache_file}"
                else:
                    cls._cache_file = cache_file
            
            cls._cache = cls._load_cache()
    
    @staticmethod
    def key(*datas, **kwargs):
        """ì—¬ëŸ¬ ë°ì´í„°ì™€ í‚¤ì›Œë“œ ì¸ìë¥¼ ë°›ì•„ì„œ ê³ ìœ í•œ í•´ì‹œí‚¤ ìƒì„±"""
        try:
            # ìœ„ì¹˜ ì¸ìë“¤ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_data = []
            for d in datas:
                if isinstance(d, np.ndarray):
                    serializable_data.append(d.tolist())
                elif isinstance(d, pd.DataFrame):
                    serializable_data.append(d.to_dict())
                elif isinstance(d, pd.Series):
                    serializable_data.append(d.to_list())
                elif hasattr(d, '__iter__') and not isinstance(d, (str, bytes)):
                    # ë¦¬ìŠ¤íŠ¸, íŠœí”Œ ë“± ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´
                    serializable_data.append(list(d))
                else:
                    serializable_data.append(d)
            
            # í‚¤ì›Œë“œ ì¸ìë“¤ì„ ì •ë ¬ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ì¶”ê°€
            if kwargs:
                serializable_data.append(dict(sorted(kwargs.items())))
            
            # JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ìƒì„±
            data_str = json.dumps(serializable_data, sort_keys=True, default=str)
            return hashlib.md5(data_str.encode()).hexdigest()
        except Exception as e:
            # ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ ê°ì²´ì˜ ë¬¸ìì—´ í‘œí˜„ìœ¼ë¡œ í´ë°±
            fallback_str = str(datas) + str(kwargs)
            return hashlib.md5(fallback_str.encode()).hexdigest()
        
    @classmethod
    def save(cls, key, value, cache_file=None):
        """ê°’ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
        cls._initialize_cache(cache_file)
        
        try:
            # í° ë°ì´í„° ì €ì¥ ì‹œ ì§„í–‰ ìƒí™© í‘œì‹œ
            data_size = sys.getsizeof(value)
            if data_size > 10 * 1024 * 1024:  # 10MB ì´ìƒ
                print(f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ì €ì¥ ì¤‘... ({data_size / 1024 / 1024:.1f}MB)")
            
            # ê°’ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_value = cls._make_serializable(value)
            cls._cache[key] = serializable_value
            cls._save_cache()
            
            if data_size > 10 * 1024 * 1024:
                print(f"ì €ì¥ ì™„ë£Œ: {key[:20]}{'...' if len(key) > 20 else ''}")
            
            return True
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    @classmethod
    def load(cls, key, cache_file=None):
        """ì €ì¥ëœ ê°’ì„ ì›ë˜ í˜•íƒœë¡œ ë³µì›í•˜ì—¬ ë°˜í™˜"""
        cls._initialize_cache(cache_file)
        
        cached_value = cls._cache.get(key, None)
        if cached_value is None:
            return None
        
        try:
            # ì €ì¥ëœ ê°’ì„ ì›ë˜ í˜•íƒœë¡œ ë³µì›
            return cls._restore_value(cached_value)
        except Exception as e:
            print(f" ë³µì› ì‹¤íŒ¨: {e}")
            return cached_value  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    @classmethod
    def _make_serializable(cls, value):
        """ê°’ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ (NumPy ë²„ì „ í˜¸í™˜ì„± ê°œì„ )"""
        if isinstance(value, np.ndarray):
            try:
                # dtype í˜¸í™˜ì„± ì²˜ë¦¬
                dtype_str = str(value.dtype)
                
                # NumPy 2.0+ í˜¸í™˜ì„±: datetime64, timedelta64 íŠ¹ë³„ ì²˜ë¦¬
                if 'datetime64' in dtype_str or 'timedelta64' in dtype_str:
                    return {
                        '_type': 'numpy_array_special',
                        'data': value.astype(str).tolist(),
                        'dtype': dtype_str,
                        'shape': value.shape,
                        'numpy_version': np.__version__
                    }
                
                # ë³µì¡í•œ dtype (object, structured) ì²˜ë¦¬
                if value.dtype == np.object_ or value.dtype.names is not None:
                    return {
                        '_type': 'numpy_array_complex',
                        'data': str(value),  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
                        'dtype': dtype_str,
                        'shape': value.shape,
                        'numpy_version': np.__version__
                    }
                
                # ì¼ë°˜ì ì¸ ê²½ìš°
                return {
                    '_type': 'numpy_array',
                    'data': value.tolist(),
                    'dtype': dtype_str,
                    'shape': value.shape,
                    'numpy_version': np.__version__
                }
                
            except Exception as e:
                # ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ í´ë°±
                return {
                    '_type': 'numpy_array_fallback',
                    'data': str(value),
                    'shape': value.shape,
                    'error': str(e),
                    'numpy_version': np.__version__
                }
        
        # NumPy ìŠ¤ì¹¼ë¼ íƒ€ì… í˜¸í™˜ì„± ê°œì„ 
        elif hasattr(value, 'dtype') and hasattr(np, 'number') and isinstance(value, np.number):
            try:
                # NumPy ìŠ¤ì¹¼ë¼ë¥¼ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                if np.issubdtype(value.dtype, np.integer):
                    return int(value)
                elif np.issubdtype(value.dtype, np.floating):
                    return float(value)
                elif np.issubdtype(value.dtype, np.complexfloating):
                    return complex(value)
                elif np.issubdtype(value.dtype, np.bool_):
                    return bool(value)
                else:
                    return value.item()  # ì¼ë°˜ì ì¸ ìŠ¤ì¹¼ë¼ ë³€í™˜
            except (ValueError, OverflowError):
                return str(value)  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ë¡œ í´ë°±
        
        elif isinstance(value, pd.DataFrame):
            return {
                '_type': 'pandas_dataframe',
                'data': value.to_dict(),
                'columns': list(value.columns),
                'index': list(value.index)
            }
        elif isinstance(value, pd.Series):
            return {
                '_type': 'pandas_series',
                'data': value.to_dict(),
                'name': value.name,
                'index': list(value.index)
            }
        elif isinstance(value, (list, tuple)):
            return [cls._make_serializable(item) for item in value]
        elif isinstance(value, dict):
            return {k: cls._make_serializable(v) for k, v in value.items()}
        else:
            return value

    @classmethod
    def _restore_value(cls, cached_value):
        """ìºì‹œëœ ê°’ì„ ì›ë˜ í˜•íƒœë¡œ ë³µì› (NumPy ë²„ì „ í˜¸í™˜ì„± ê°œì„ )"""
        if isinstance(cached_value, dict) and '_type' in cached_value:
            if cached_value['_type'] == 'numpy_array':
                try:
                    dtype_str = cached_value['dtype']
                    
                    # dtype ë¬¸ìì—´ ì •ê·œí™” (ë²„ì „ í˜¸í™˜ì„±)
                    if dtype_str.startswith('<') or dtype_str.startswith('>'):
                        # ì—”ë””ì•ˆ ì •ë³´ ì œê±°
                        dtype_str = dtype_str[1:]
                    
                    # ì•ˆì „í•œ ë°°ì—´ ìƒì„±
                    arr = np.array(cached_value['data'], dtype=dtype_str)
                    return arr.reshape(cached_value['shape'])
                    
                except (ValueError, TypeError) as e:
                    try:
                        # í˜¸í™˜ ëª¨ë“œ: dtype ì¶”ë¡ í•˜ì—¬ ìƒì„±
                        arr = np.array(cached_value['data'])
                        return arr.reshape(cached_value['shape'])
                    except Exception:
                        return cached_value['data']
            
            elif cached_value['_type'] == 'numpy_array_special':
                try:
                    # íŠ¹ë³„ ì²˜ë¦¬ëœ ë°°ì—´ ë³µì›
                    arr = np.array(cached_value['data'])
                    return arr.reshape(cached_value['shape'])
                except Exception:
                    return cached_value['data']
            
            elif cached_value['_type'] in ['numpy_array_complex', 'numpy_array_fallback']:
                # ë³µì¡í•œ dtypeì´ë‚˜ í´ë°±ëœ ê²½ìš° ë¬¸ìì—´ í‘œí˜„ë§Œ ë°˜í™˜
                return cached_value['data']
            
            elif cached_value['_type'] == 'pandas_dataframe':
                return pd.DataFrame(cached_value['data'], columns=cached_value['columns'], index=cached_value['index'])
            elif cached_value['_type'] == 'pandas_series':
                return pd.Series(cached_value['data'], name=cached_value['name'], index=cached_value['index'])
        
        elif isinstance(cached_value, list):
            return [cls._restore_value(item) for item in cached_value]
        elif isinstance(cached_value, dict):
            return {k: cls._restore_value(v) for k, v in cached_value.items()}
        
        return cached_value

    @classmethod
    def _load_cache(cls):
        """ìºì‹œ íŒŒì¼ ë¡œë“œ (ë°±ì—… ì‹œìŠ¤í…œ ì ìš©)"""
        backup_file = cls._cache_file + ".bak"
        
        # ë©”ì¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹œë„
        if os.path.exists(cls._cache_file):
            try:
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(cls._cache_file)
                if file_size > 100 * 1024 * 1024:  # 100MB ì´ìƒ
                    print(f"ê²½ê³ : ìºì‹œ íŒŒì¼ì´ ë§¤ìš° í½ë‹ˆë‹¤ ({file_size / 1024 / 1024:.1f}MB). ë¡œë”©ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # í° íŒŒì¼ì„ ìœ„í•œ ì²­í¬ ë‹¨ìœ„ ì½ê¸°
                with open(cls._cache_file, "r", encoding='utf-8', buffering=8192) as f:
                    # JSON íŒŒì¼ì˜ ì™„ì „ì„± ê²€ì¦ì„ ìœ„í•´ ëê¹Œì§€ ì½ê¸°
                    content = f.read()
                    if not content.strip():
                        print("ìºì‹œ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        return {}
                    
                    # JSON íŒŒì‹±
                    cache_data = json.loads(content)
                    print(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(cache_data)}ê°œ í•­ëª© ({file_size / 1024 / 1024:.2f}MB)")
                    return cache_data
                    
            except json.JSONDecodeError as e:
                print(f"ì˜¤ë¥˜: ìºì‹œ íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
                return cls._load_from_backup()
            except MemoryError:
                print(f"ì˜¤ë¥˜: ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   íŒŒì¼ í¬ê¸°: {file_size / 1024 / 1024:.1f}MB")
                return cls._load_from_backup()
            except Exception as e:
                print(f"ì˜¤ë¥˜: ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return cls._load_from_backup()
        
        # ë©”ì¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë°±ì—… íŒŒì¼ í™•ì¸
        elif os.path.exists(backup_file):
            print("ë©”ì¸ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë°±ì—… íŒŒì¼ì—ì„œ ë³µì›ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            return cls._load_from_backup()
        
        return {}
    
    @classmethod
    def _load_from_backup(cls):
        """ë°±ì—… íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
        backup_file = cls._cache_file + ".bak"
        
        if not os.path.exists(backup_file):
            print("ë°±ì—… íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            print("ë°±ì—… íŒŒì¼ì—ì„œ ìºì‹œë¥¼ ë³µì›í•˜ëŠ” ì¤‘...")
            
            with open(backup_file, "r", encoding='utf-8', buffering=8192) as f:
                content = f.read()
                if not content.strip():
                    print("ë°±ì—… íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return {}
                
                cache_data = json.loads(content)
            
            # ì†ìƒëœ ë©”ì¸ íŒŒì¼ ì‚­ì œ
            if os.path.exists(cls._cache_file):
                corrupted_file = cls._cache_file + ".corrupted"
                try:
                    os.rename(cls._cache_file, corrupted_file)
                    print(f"ì†ìƒëœ ìºì‹œ íŒŒì¼ì„ {corrupted_file}ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
                except:
                    try:
                        os.remove(cls._cache_file)
                        print("ì†ìƒëœ ìºì‹œ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                    except:
                        pass
            
            # ë°±ì—… íŒŒì¼ì„ ë©”ì¸ íŒŒì¼ë¡œ ë³µì‚¬
            try:
                shutil.copy2(backup_file, cls._cache_file)
                print("ë°±ì—… íŒŒì¼ì—ì„œ ë©”ì¸ ìºì‹œ íŒŒì¼ì„ ë³µì›í–ˆìŠµë‹ˆë‹¤.")
                print("ì£¼ì˜: ìºì‹œê°€ ì´ì „ ìƒíƒœë¡œ ë˜ëŒë ¤ì¡ŒìŠµë‹ˆë‹¤. ì¼ë¶€ ìµœê·¼ ë°ì´í„°ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"ë°±ì—… íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            
            backup_size = os.path.getsize(backup_file)
            print(f"ë°±ì—…ì—ì„œ ìºì‹œ ë³µì› ì™„ë£Œ: {len(cache_data)}ê°œ í•­ëª© ({backup_size / 1024 / 1024:.2f}MB)")
            return cache_data
            
        except json.JSONDecodeError as e:
            print(f"ì˜¤ë¥˜: ë°±ì—… íŒŒì¼ë„ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
            return {}
        except Exception as e:
            print(f"ì˜¤ë¥˜: ë°±ì—… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    @classmethod
    def _cleanup_temp_files(cls):
        """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
        temp_file = cls._cache_file + ".tmp"
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass

    @classmethod
    def _save_cache(cls):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥ (ë°±ì—… ì‹œìŠ¤í…œ ì ìš©)"""
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            cache_dir = os.path.dirname(cls._cache_file)
            if cache_dir and not os.path.exists(cache_dir):
                os.makedirs(cache_dir, exist_ok=True)
            
            # íŒŒì¼ ê²½ë¡œ ì„¤ì •
            temp_file = cls._cache_file + ".tmp"
            backup_file = cls._cache_file + ".bak"
            
            # ì˜ˆìƒ íŒŒì¼ í¬ê¸° ì¶”ì •
            cache_str = json.dumps(cls._cache, indent=2, ensure_ascii=False)
            estimated_size = len(cache_str.encode('utf-8'))
            
            if estimated_size > 50 * 1024 * 1024:  # 50MB ì´ìƒ
                print(f"ê²½ê³ : í° ìºì‹œ íŒŒì¼ ì €ì¥ ì¤‘... (ì˜ˆìƒ í¬ê¸°: {estimated_size / 1024 / 1024:.1f}MB)")
            
            # ì„ì‹œ íŒŒì¼ì— ì €ì¥
            with open(temp_file, "w", encoding='utf-8', buffering=8192) as f:
                # ëŒ€ìš©ëŸ‰ JSONì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì‘ì„±
                if estimated_size > 10 * 1024 * 1024:  # 10MB ì´ìƒ
                    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ JSON ì €ì¥
                    f.write('{\n')
                    items = list(cls._cache.items())
                    for i, (key, value) in enumerate(items):
                        f.write(f'  {json.dumps(key, ensure_ascii=False)}: ')
                        f.write(json.dumps(value, indent=2, ensure_ascii=False).replace('\n', '\n  '))
                        if i < len(items) - 1:
                            f.write(',')
                        f.write('\n')
                        
                        # ì£¼ê¸°ì ìœ¼ë¡œ í”ŒëŸ¬ì‹œ
                        if i % 100 == 0:
                            f.flush()
                    f.write('}')
                else:
                    # ì¼ë°˜ì ì¸ ê²½ìš°
                    f.write(cache_str)
                
                f.flush()  # ë²„í¼ ê°•ì œ í”ŒëŸ¬ì‹œ
                os.fsync(f.fileno())  # ë””ìŠ¤í¬ì— ê°•ì œ ë™ê¸°í™”
            
            # ì„ì‹œ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ ê²€ì¦
            try:
                with open(temp_file, "r", encoding='utf-8') as f:
                    json.load(f)  # JSON íŒŒì‹± í…ŒìŠ¤íŠ¸
            except:
                print("ì˜¤ë¥˜: ì„ì‹œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                return False
            
            # ë°±ì—… ì‹œìŠ¤í…œ ì ìš©
            # 1. ê¸°ì¡´ ë°±ì—… íŒŒì¼ ì‚­ì œ
            if os.path.exists(backup_file):
                os.remove(backup_file)
            
            # 2. ê¸°ì¡´ ìºì‹œ íŒŒì¼ì„ ë°±ì—…ìœ¼ë¡œ ì´ë™ (ìˆëŠ” ê²½ìš°)
            if os.path.exists(cls._cache_file):
                os.rename(cls._cache_file, backup_file)
            
            # 3. ì„ì‹œ íŒŒì¼ì„ ë©”ì¸ ìºì‹œ íŒŒì¼ë¡œ ì´ë™
            os.rename(temp_file, cls._cache_file)
            
            # ì €ì¥ ì™„ë£Œ í™•ì¸
            actual_size = os.path.getsize(cls._cache_file)
            if estimated_size > 10 * 1024 * 1024:
                print(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(cls._cache)}ê°œ í•­ëª© ({actual_size / 1024 / 1024:.2f}MB)")
            
            return True
                
        except OSError as e:
            print(f"ì˜¤ë¥˜: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ë˜ëŠ” ê¶Œí•œ ì˜¤ë¥˜: {e}")
            print(f"ê²½ë¡œ: {cls._cache_file}")
            cls._cleanup_temp_files()
            return False
        except MemoryError:
            print(f"ì˜¤ë¥˜: ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìºì‹œë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ìºì‹œ í•­ëª© ìˆ˜: {len(cls._cache)}")
            cls._cleanup_temp_files()
            return False
        except Exception as e:
            print(f"ì˜¤ë¥˜: ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"ê²½ë¡œ: {cls._cache_file}")
            if _in_colab():
                print("Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            cls._cleanup_temp_files()
            return False

    @classmethod
    def clear_cache(cls, cache_file=None):
        """ìºì‹œ ì´ˆê¸°í™”"""
        cls._initialize_cache(cache_file)
        cls._cache = {}
        if os.path.exists(cls._cache_file):
            os.remove(cls._cache_file)

    @classmethod
    def cache_info(cls, cache_file=None):
        """ìºì‹œ ì •ë³´ ì¶œë ¥"""
        cls._initialize_cache(cache_file)
        env_name = "Colab" if _in_colab() else "ë¡œì»¬"
        print(f"ìºì‹œ ì •ë³´ ({env_name} í™˜ê²½):")
        print(f"   - íŒŒì¼: {cls._cache_file}")
        print(f"   - í•­ëª© ìˆ˜: {len(cls._cache):,}")
        
        if os.path.exists(cls._cache_file):
            file_size = os.path.getsize(cls._cache_file)
            size_mb = file_size / 1024 / 1024
            
            if size_mb >= 1:
                print(f"   - íŒŒì¼ í¬ê¸°: {size_mb:.2f}MB ({file_size:,} bytes)")
            elif file_size >= 1024:
                print(f"   - íŒŒì¼ í¬ê¸°: {file_size / 1024:.1f}KB ({file_size:,} bytes)")
            else:
                print(f"   - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            try:
                cache_memory = sys.getsizeof(cls._cache)
                for key, value in cls._cache.items():
                    cache_memory += sys.getsizeof(key) + sys.getsizeof(value)
                
                memory_mb = cache_memory / 1024 / 1024
                if memory_mb >= 1:
                    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ {memory_mb:.2f}MB")
                else:
                    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ {cache_memory / 1024:.1f}KB")
            except:
                pass
            
            # í° íŒŒì¼ì— ëŒ€í•œ ê²½ê³ 
            if size_mb > 50:
                print(f"   ê²½ê³ : ìºì‹œ íŒŒì¼ì´ í½ë‹ˆë‹¤. ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif size_mb > 10:
                print(f"   ì ë‹¹í•œ í¬ê¸°ì˜ ìºì‹œ íŒŒì¼ì…ë‹ˆë‹¤.")
                
        else:
            print(f"   - ìƒíƒœ: ìºì‹œ íŒŒì¼ ì—†ìŒ")
        
        # ìµœê·¼ ìˆ˜ì • ì‹œê°„
        if os.path.exists(cls._cache_file):
            mtime = os.path.getmtime(cls._cache_file)
            mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mtime))
            print(f"   - ìµœê·¼ ìˆ˜ì •: {mtime_str}")

    @classmethod
    def delete(cls, key, cache_file=None):
        """íŠ¹ì • í‚¤ ì‚­ì œ"""
        cls._initialize_cache(cache_file)
        
        if key in cls._cache:
            del cls._cache[key]
            cls._save_cache()
            print(f" í‚¤ '{key}' ì‚­ì œ ì™„ë£Œ")
            return True
        else:
            print(f" í‚¤ '{key}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
    
    @classmethod
    def delete_keys(cls, *keys, cache_file=None):
        """ì—¬ëŸ¬ í‚¤ë¥¼ í•œë²ˆì— ì‚­ì œ"""
        cls._initialize_cache(cache_file)
        
        deleted_count = 0
        for key in keys:
            if key in cls._cache:
                del cls._cache[key]
                deleted_count += 1
                print(f" í‚¤ '{key}' ì‚­ì œ")
            else:
                print(f" í‚¤ '{key}' ì—†ìŒ")
        
        if deleted_count > 0:
            cls._save_cache()
            print(f" ì´ {deleted_count}ê°œ í‚¤ ì‚­ì œ ì™„ë£Œ")
        
        return deleted_count
    
    @classmethod
    def list_keys(cls, cache_file=None):
        """ì €ì¥ëœ ëª¨ë“  í‚¤ ëª©ë¡ ì¡°íšŒ"""
        cls._initialize_cache(cache_file)
        return list(cls._cache.keys())
    
    @classmethod
    def exists(cls, key, cache_file=None):
        """í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        cls._initialize_cache(cache_file)
        return key in cls._cache
    
    @classmethod
    def size(cls, cache_file=None):
        """ìºì‹œ í¬ê¸° ë°˜í™˜"""
        cls._initialize_cache(cache_file)
        return len(cls._cache)
    
    @classmethod
    def compress_cache(cls, cache_file=None):
        """ìºì‹œ íŒŒì¼ ì••ì¶•í•˜ì—¬ ì €ì¥ ê³µê°„ ì ˆì•½"""
        cls._initialize_cache(cache_file)
        
        if not os.path.exists(cls._cache_file):
            print("ì••ì¶•í•  ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            
            original_size = os.path.getsize(cls._cache_file)
            compressed_file = cls._cache_file + ".gz"
            
            print(f"ìºì‹œ íŒŒì¼ ì••ì¶• ì¤‘... (ì›ë³¸: {original_size / 1024 / 1024:.2f}MB)")
            
            with open(cls._cache_file, 'rb') as f_in:
                with gzip.open(compressed_file, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            compressed_size = os.path.getsize(compressed_file)
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            print(f"ì••ì¶• ì™„ë£Œ: {compressed_size / 1024 / 1024:.2f}MB")
            print(f"ì••ì¶•ë¥ : {compression_ratio:.1f}% ì ˆì•½")
            print(f"ì••ì¶• íŒŒì¼: {compressed_file}")
            
            return True
            
        except ImportError:
            print("ì˜¤ë¥˜: gzip ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì••ì¶• ì‹¤íŒ¨: {e}")
            return False
    
    @classmethod
    def cleanup_cache(cls, days=30, cache_file=None):
        """ìºì‹œ ì •ë¦¬ (í˜„ì¬ëŠ” ìˆ˜ë™ ì •ë¦¬)"""
        cls._initialize_cache(cache_file)
        
        if not cls._cache:
            print("ì •ë¦¬í•  ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        print(f"ìºì‹œ ì •ë¦¬ ë„êµ¬ (í˜„ì¬ {len(cls._cache)}ê°œ í•­ëª©)")
        print("í–¥í›„ ì—…ë°ì´íŠ¸ì—ì„œ ìë™ ì •ë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤.")
        print("í˜„ì¬ëŠ” ìˆ˜ë™ìœ¼ë¡œ cache_clear() ë˜ëŠ” cache_delete() ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í° í•­ëª©ë“¤ í‘œì‹œ
        try:
            large_items = []
            for key, value in cls._cache.items():
                item_size = sys.getsizeof(value)
                if item_size > 1024 * 1024:  # 1MB ì´ìƒ
                    large_items.append((key, item_size))
            
            if large_items:
                large_items.sort(key=lambda x: x[1], reverse=True)
                print("\ní° ìºì‹œ í•­ëª©ë“¤ (1MB ì´ìƒ):")
                for key, size in large_items[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    print(f"  - {key[:50]}{'...' if len(key) > 50 else ''}: {size / 1024 / 1024:.2f}MB")
                    
        except Exception:
            pass
        
        return len(cls._cache)
    
    @classmethod
    def optimize_cache(cls, cache_file=None):
        """ìºì‹œ ìµœì í™” (ì¬ì €ì¥ìœ¼ë¡œ íŒŒì¼ í¬ê¸° ìµœì í™”)"""
        cls._initialize_cache(cache_file)
        
        if not os.path.exists(cls._cache_file):
            print("ìµœì í™”í•  ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            original_size = os.path.getsize(cls._cache_file)
            print(f"ìºì‹œ íŒŒì¼ ìµœì í™” ì¤‘... (í˜„ì¬: {original_size / 1024 / 1024:.2f}MB)")
            
            # ìºì‹œë¥¼ ë‹¤ì‹œ ì €ì¥í•˜ì—¬ íŒŒì¼ ìµœì í™”
            cls._save_cache()
            
            new_size = os.path.getsize(cls._cache_file)
            if new_size < original_size:
                saved_size = original_size - new_size
                saved_percent = (saved_size / original_size) * 100
                print(f"ìµœì í™” ì™„ë£Œ: {saved_size / 1024 / 1024:.2f}MB ì ˆì•½ ({saved_percent:.1f}%)")
            else:
                print("ìµœì í™” ì™„ë£Œ: ì¶”ê°€ ì ˆì•½ ê³µê°„ ì—†ìŒ")
            
            return True
            
        except Exception as e:
            print(f"ì˜¤ë¥˜: ìµœì í™” ì‹¤íŒ¨: {e}")
            return False


def _generate_commit_hash(dt, msg):
    """ì»¤ë°‹ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    base = f"{dt.strftime('%Y%m%d_%H%M%S')}_{msg}"
    return hashlib.md5(base.encode("utf-8")).hexdigest()[:12]

def df_to_pickle(df, path):
    """
    DataFrameê³¼ df.attrs(ë”•ì…”ë„ˆë¦¬)ê¹Œì§€ í•¨ê»˜ pickleë¡œ ì €ì¥
    """
    obj = {
        "data": df,
        "attrs": getattr(df, 'attrs', {})
    }
    with open(path, "wb") as f:
        pickle.dump(obj, f)

def df_read_pickle(path):
    """
    DataFrameê³¼ attrs(ë”•ì…”ë„ˆë¦¬)ê¹Œì§€ ë³µì›
    """
    with open(path, "rb") as f:
        obj = pickle.load(f)
    df = obj["data"]
    if "attrs" in obj:
        df.attrs = obj["attrs"]
    return df


# =============================================================================
# PANDAS COMMIT SYSTEM: CORE FUNCTIONS
# =============================================================================

def pd_commit(df, msg, commit_dir=None):
    """
    DataFrameì˜ í˜„ì¬ ìƒíƒœë¥¼ gitì²˜ëŸ¼ ì»¤ë°‹í•©ë‹ˆë‹¤.
    íŒŒì¼ëª…: í•´ì‹œí‚¤.pkl, ë©”íƒ€: pandas_df.json
    commit_dir: ì €ì¥í•  í´ë” ì§€ì • (Noneì´ë©´ ê¸°ë³¸)
    ë™ì¼í•œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ì»¤ë°‹ì„ ìƒˆ ì»¤ë°‹ìœ¼ë¡œ ëŒ€ì²´(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤.
    """
    if df is None or not isinstance(df, pd.DataFrame):
        raise ValueError("df ì¸ìê°€ Noneì´ê±°ë‚˜ ìœ íš¨í•œ DataFrameì´ ì•„ë‹™ë‹ˆë‹¤.")
    dt = datetime.datetime.now()
    dt_str = dt.strftime("%Y-%m-%d %H:%M:%S")  # ISO8601 í¬ë§·
    commit_hash = _generate_commit_hash(dt, msg)
    fname = f"{commit_hash}.pkl_helper"
    save_dir = os.path.join(pd_root(commit_dir), ".commit_pandas")
    os.makedirs(save_dir, exist_ok=True)

    meta = _load_commit_meta(commit_dir)
    # print(f"ì»¤ë°‹ ì¤€ë¹„: {commit_hash} | {dt_str} | {msg}")
    # print(f"meta: {meta}")
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    
    # ë™ì¼í•œ ë©”ì‹œì§€(msg)ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ë° ë©”íƒ€ì—ì„œ ì œê±°
    old_idx = None
    for i, m in enumerate(meta):
        if m["msg"] == msg:
            old_file = os.path.join(save_dir, m["file"])
            if os.path.exists(old_file):
                os.remove(old_file)
            old_idx = i
            break
    if old_idx is not None:
        meta.pop(old_idx)

    # ìƒˆ ì»¤ë°‹ ì €ì¥
    df_to_pickle(df, os.path.join(save_dir, fname))
    meta.append({
        "hash": commit_hash,
        "datetime": dt_str,
        "msg": msg,
        "file": fname
    })
    _save_commit_meta(meta, commit_dir)
    print(f"âœ… ì»¤ë°‹ ì™„ë£Œ: {commit_hash} | {dt_str} | {msg}")
    return df


def pd_commit_list(commit_dir=None):
    """
    ì»¤ë°‹ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ë°˜í™˜ (ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ, ì—†ìœ¼ë©´ ìë™ ì‚­ì œ)
    commit_dir: ì €ì¥ í´ë” ì§€ì •
    ë°˜í™˜ê°’: pandas.DataFrame (ìˆœì„œ, í•´ì‹œ, ì‹œê°„, ë©”ì‹œì§€, íŒŒì¼)
    """
    meta = _load_commit_meta(commit_dir)
    save_dir = os.path.join(pd_root(commit_dir), ".commit_pandas")
    new_meta = []
    removed_count = 0
    
    for m in meta:
        file_path = os.path.join(save_dir, m["file"])
        if os.path.exists(file_path):
            new_meta.append(m)
        else:
            print(f"ê²½ê³ : ëˆ„ë½ëœ íŒŒì¼ '{m['file']}' (ë©”ì‹œì§€: {m['msg']}) ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°")
            removed_count += 1
    
    # ë©”íƒ€ë°ì´í„° ì •ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
    if removed_count > 0:
        _save_commit_meta(new_meta, commit_dir)
        print(f"âœ… {removed_count}ê°œì˜ ëˆ„ë½ëœ ì»¤ë°‹ í•­ëª©ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    
    new_meta.sort(key=lambda x: x["datetime"])
    # DataFrame ë³€í™˜
    df = pd.DataFrame(new_meta)
    if not df.empty:
        # datetime ì»¬ëŸ¼ì„ pandas datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df.insert(0, 'index', range(len(df)))
    else:
        print("ì»¤ë°‹ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
    return df

def pd_checkout(idx_or_hash, commit_dir=None):
    """
    ì»¤ë°‹ í•´ì‹œ, ì‹œê°„ì •ë³´, ë©”ì‹œì§€, ìˆœì„œë²ˆí˜¸ë¡œ DataFrame ë³µì›
    íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ë©”íƒ€ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ê³  ë¹ˆ DataFrame ë°˜í™˜
    commit_dir: ì €ì¥ í´ë” ì§€ì •
    """
    meta = _load_commit_meta(commit_dir)
    save_dir = os.path.join(pd_root(commit_dir), ".commit_pandas")
    
    # ë©”íƒ€ë°ì´í„° ì •ë¦¬ í”Œë˜ê·¸
    meta_updated = False
    
    if isinstance(idx_or_hash, int):
        if idx_or_hash < 0 or idx_or_hash >= len(meta):
            print(f"ì˜¤ë¥˜: ìˆœì„œë²ˆí˜¸ {idx_or_hash}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (0-{len(meta)-1})")
            return pd.DataFrame()
        
        fname = meta[idx_or_hash]["file"]
        file_path = os.path.join(save_dir, fname)
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(file_path):
            print(f"ê²½ê³ : íŒŒì¼ '{fname}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.")
            meta.pop(idx_or_hash)
            _save_commit_meta(meta, commit_dir)
            return pd.DataFrame()
        
        try:
            return df_read_pickle(file_path)
        except Exception as e:
            print(f"ì˜¤ë¥˜: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            # ì†ìƒëœ íŒŒì¼ ì •ë³´ë¥¼ ë©”íƒ€ì—ì„œ ì œê±°
            meta.pop(idx_or_hash)
            _save_commit_meta(meta, commit_dir)
            return pd.DataFrame()
    
    # í•´ì‹œ, ë‚ ì§œ, ë©”ì‹œì§€ë¡œ ê²€ìƒ‰
    for i, m in enumerate(meta):
        if idx_or_hash == m["hash"] or idx_or_hash == m["datetime"] or idx_or_hash == m["msg"]:
            fname = m["file"]
            file_path = os.path.join(save_dir, fname)
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(file_path):
                print(f"ê²½ê³ : íŒŒì¼ '{fname}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.")
                meta.pop(i)
                _save_commit_meta(meta, commit_dir)
                return pd.DataFrame()
            
            try:
                return df_read_pickle(file_path)
            except Exception as e:
                print(f"ì˜¤ë¥˜: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                # ì†ìƒëœ íŒŒì¼ ì •ë³´ë¥¼ ë©”íƒ€ì—ì„œ ì œê±°
                meta.pop(i)
                _save_commit_meta(meta, commit_dir)
                return pd.DataFrame()
    
    print(f"ì˜¤ë¥˜: ì»¤ë°‹ '{idx_or_hash}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜


def pd_commit_rm(idx_or_hash, commit_dir=None):
    """
    ì»¤ë°‹ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    idx_or_hash: ì‚­ì œí•  ì»¤ë°‹ì˜ ì¸ë±ìŠ¤, í•´ì‹œ, ë‚ ì§œ, ë˜ëŠ” ë©”ì‹œì§€
    commit_dir: ì €ì¥ í´ë” ì§€ì •
    """
    meta = _load_commit_meta(commit_dir)
    save_dir = os.path.join(pd_root(commit_dir), ".commit_pandas")
    if isinstance(idx_or_hash, int):
        if idx_or_hash < 0 or idx_or_hash >= len(meta):
            print(f"ì˜¤ë¥˜: ìˆœì„œë²ˆí˜¸ {idx_or_hash}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (0-{len(meta)-1})")
            return False
        fname = meta[idx_or_hash]["file"]
        try:
            os.remove(os.path.join(save_dir, fname))
            meta.pop(idx_or_hash)  # ë©”íƒ€ì—ì„œ ì‚­ì œ
            _save_commit_meta(meta, commit_dir)
            print(f"âœ… ì»¤ë°‹ {idx_or_hash} ì‚­ì œ ì™„ë£Œ")
            return True
        except OSError as e:
            print(f"ì˜¤ë¥˜: íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    for i, m in enumerate(meta):
        if idx_or_hash == m["hash"] or idx_or_hash == m["datetime"] or idx_or_hash == m["msg"]:
            fname = m["file"]
            try:
                os.remove(os.path.join(save_dir, fname))
                meta.pop(i)  # ë©”íƒ€ì—ì„œ ì‚­ì œ
                _save_commit_meta(meta, commit_dir)
                print(f"âœ… ì»¤ë°‹ '{idx_or_hash}' ì‚­ì œ ì™„ë£Œ")
                return True
            except OSError as e:
                print(f"ì˜¤ë¥˜: íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                return False
    print(f"ì˜¤ë¥˜: ì»¤ë°‹ '{idx_or_hash}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return False

def pd_commit_has(idx_or_hash, commit_dir=None):
    """
    ì»¤ë°‹ index, hash, datetime, msg ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥ë°›ì•„
    í•´ë‹¹ ì»¤ë°‹ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ True, ì—†ìœ¼ë©´ False ë°˜í™˜
    """
    meta = _load_commit_meta(commit_dir)
    save_dir = os.path.join(pd_root(commit_dir), ".commit_pandas")
    # indexë¡œ ê²€ì‚¬
    if isinstance(idx_or_hash, int):
        if 0 <= idx_or_hash < len(meta):
            fname = meta[idx_or_hash]["file"]
            if os.path.exists(os.path.join(save_dir, fname)):
                return True
        return False
    # hash, datetime, msgë¡œ ê²€ì‚¬
    for m in meta:
        if idx_or_hash == m["hash"] or idx_or_hash == m["datetime"] or idx_or_hash == m["msg"]:
            fname = m["file"]
            if os.path.exists(os.path.join(save_dir, fname)):
                return True
    return False

#########################################################################################################
class AIHubShell:
    def __init__(self, debug=False, download_dir=None):
        self.BASE_URL = "https://api.aihub.or.kr"
        self.LOGIN_URL = f"{self.BASE_URL}/api/keyValidate.do"
        self.BASE_DOWNLOAD_URL = f"{self.BASE_URL}/down/0.5"
        self.MANUAL_URL = f"{self.BASE_URL}/info/api.do"
        self.BASE_FILETREE_URL = f"{self.BASE_URL}/info"
        self.DATASET_URL = f"{self.BASE_URL}/info/dataset.do"
        self.debug = debug
        self.download_dir = download_dir if download_dir else "."
                
    def help(self):
        """ì‚¬ìš©ë²• ì¶œë ¥"""
        print("AIHubShell í´ë˜ìŠ¤ ì‚¬ìš©ë²•")
        print("- ì¸ìŠ¤í„´ìŠ¤ ìƒì„±: AIHubShell(debug=False, download_dir=None)")
        print("  * debug: Trueë¡œ ì„¤ì •í•˜ë©´ API ì›ë³¸ ì‘ë‹µ ë“± ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
        print("  * download_dir: ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì • (ê¸°ë³¸ê°’: í˜„ì¬ ê²½ë¡œ)")
        print("- ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ: list_info() ë˜ëŠ” list_search(datasetname='ê²€ìƒ‰ì–´')")
        print("- íŠ¹ì • ë°ì´í„°ì…‹ íŠ¸ë¦¬ ì¡°íšŒ: list_info(datasetkey=ìˆ«ì)")
        print("- íŠ¹ì • ì´ë¦„ í¬í•¨ ë°ì´í„°ì…‹ íŠ¸ë¦¬ ì¡°íšŒ: list_info(datasetname='ê²€ìƒ‰ì–´')")
        print("- ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ: download_dataset(apikey, datasetkey, filekeys='all')")
        print()
        print("ì˜ˆì‹œ:")
        print("  aihub = AIHubShell(debug=True, download_dir='./data')")
        print("  aihub.list_info()")
        print("  aihub.list_search(datasetname='ê²½êµ¬ì•½ì œ')")
        print("  aihub.list_info(datasetkey=576)")
        print("  aihub.download_dataset(apikey='APIí‚¤', datasetkey=576, filekeys='66065')")
        print()
        print("ìì„¸í•œ API ì„¤ëª…ì€ aihub.print_usage() ë˜ëŠ” ê³µì‹ ë¬¸ì„œ ì°¸ê³ ")
                        
    def print_usage(self):
        """ì‚¬ìš©ë²• ì¶œë ¥"""
        try:
            response = requests.get(self.MANUAL_URL)
            manual = response.text
            
            if self.debug:
                print("API ì›ë³¸ ì‘ë‹µ:")
                print(manual)            
            
            # JSON íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            try:
                manual = re.sub(r'("FRST_RGST_PNTTM":)([0-9\- :\.]+)', r'\1"\2"', manual)
                manual_data = json.loads(manual)
                if self.debug:
                    print("JSON íŒŒì‹± ì„±ê³µ")
                    
                if 'result' in manual_data and len(manual_data['result']) > 0:
                    print(manual_data['result'][0].get('SJ', ''))
                    print()
                    print("ENGL_CMGG\t KOREAN_CMGG\t\t\t DETAIL_CN")
                    print("-" * 80)
                    
                    for item in manual_data['result']:
                        engl = item.get('ENGL_CMGG', '')
                        korean = item.get('KOREAN_CMGG', '')
                        detail = item.get('DETAIL_CN', '').replace('\\n', '\n').replace('\\t', '\t')
                        print(f"{engl:<10}\t {korean:<15}\t|\t {detail}\n")
            except json.JSONDecodeError:
                if self.debug:
                    print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
                else:
                    print("API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜")
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
    
    def merge_parts(self, target_dir):
        """part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        target_path = Path(target_dir)
        part_files = list(target_path.glob("*.part*"))
        
        if not part_files:
            return
            
        # prefixë³„ë¡œ ê·¸ë£¹í™”
        prefixes = {}
        for part_file in part_files:
            match = re.match(r'(.+)\.part(\d+)$', part_file.name)
            if match:
                prefix = match.group(1)
                part_num = int(match.group(2))
                if prefix not in prefixes:
                    prefixes[prefix] = []
                prefixes[prefix].append((part_num, part_file))
        
        # ê° prefixë³„ë¡œ ë³‘í•©
        for prefix, parts in prefixes.items():
            print(f"Merging {prefix} in {target_dir}")
            parts.sort(key=lambda x: x[0])  # part ë²ˆí˜¸ë¡œ ì •ë ¬
            
            output_path = target_path / prefix
            with open(output_path, 'wb') as output_file:
                for _, part_file in parts:
                    with open(part_file, 'rb') as input_file:
                        shutil.copyfileobj(input_file, output_file)
            
            # part íŒŒì¼ë“¤ ì‚­ì œ
            for _, part_file in parts:
                part_file.unlink()
                
    def merge_all_parts(self, base_path="."):
        """ëª¨ë“  í•˜ìœ„ í´ë”ì˜ part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        print("ë³‘í•© ì¤‘ì…ë‹ˆë‹¤...")
        for root, dirs, files in os.walk(base_path):
            part_files = [f for f in files if '.part' in f]
            if part_files:
                self.merge_parts(root)
        print("ë³‘í•©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def download_dataset(self, apikey, datasetkey, filekeys="all"):
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        download_path = Path(self.download_dir)
        download_tar_path = download_path / "download.tar"

        # ê¸°ì¡´ download.tar ë°±ì—…
        if download_tar_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = download_path / f"download_{timestamp}.tar"
            shutil.move(str(download_tar_path), str(backup_name))
            print(f"msg : download.tar íŒŒì¼ì´ ì¡´ì¬í•˜ì—¬ {backup_name}ë¡œ ë°±ì—…í•˜ì˜€ìŠµë‹ˆë‹¤.")

        def cleanup_handler(signum, frame):
            if download_tar_path.exists():
                download_tar_path.unlink()
                print("\në‹¤ìš´ë¡œë“œê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

        signal.signal(signal.SIGINT, cleanup_handler)

        download_url = f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do"
        headers = {"apikey": apikey}
        params = {"fileSn": filekeys}

        try:
            print("ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            os.makedirs(download_path, exist_ok=True)
            response = requests.get(download_url, headers=headers, params=params, stream=True)

            if response.status_code == 200:
                print(f"Request successful with HTTP status {response.status_code}.")
                with open(download_tar_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print("Download successful.")

                # tar íŒŒì¼ í•´ì œ
                print("ì••ì¶• í•´ì œ ì¤‘...")
                with tarfile.open(download_tar_path, "r") as tar:
                    tar.extractall(path=download_path)

                # part íŒŒì¼ë“¤ ë³‘í•©
                self.merge_all_parts(download_path)

                # download.tar ì‚­ì œ
                download_tar_path.unlink()
                print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            else:
                print(f"Download failed with HTTP status {response.status_code}.")
                print("Error msg:")
                print(response.text)
                if download_tar_path.exists():
                    download_tar_path.unlink()
        except requests.RequestException as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            if download_tar_path.exists():
                download_tar_path.unlink()
    
    # filepath: [ê²½êµ¬ì•½ì œ_ì´ë¯¸ì§€_ë°ì´í„°.ipynb](http://_vscodecontentref_/0)
    def list_info(self, datasetkey=None, datasetname=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ"""
        if datasetkey:
            filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"
            print("Fetching file tree structure...")
            try:
                response = requests.get(filetree_url)
                # ì¸ì½”ë”© ìë™ ê°ì§€
                response.encoding = response.apparent_encoding
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        else:
            print("Fetching dataset information...")
            try:
                response = requests.get(self.DATASET_URL)
                response.encoding = 'utf-8'
                #response.encoding = 'euc-kr'
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")


    def list_search(self, datasetname=None, tree=False):
        """
        ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŠ¹ì • ì´ë¦„ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
        datasetname: ê²€ìƒ‰í•  ë°ì´í„°ì…‹ ì´ë¦„ (ë¶€ë¶„ ì¼ì¹˜)
        tree: Trueì´ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ë„ ì¡°íšŒ        
        """
        print("Fetching dataset information...")
        try:
            response = requests.get(self.DATASET_URL)
            response.encoding = 'utf-8'
            text = response.text
            if datasetname:
                # datasetnameì´ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì¶œë ¥
                lines = text.splitlines()
                for line in lines:
                    if datasetname in line:
                        #print(line)
                        # 576, ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°
                        num, name = line.split(',', 1)
                        # í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
                        if tree:
                            self.list_info(datasetkey=int(num.strip()))
                        else:
                            print(line)
            else:
                print(text)
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
            
#########################################################################################################

# ëª¨ë“ˆ import ì‹œ ìë™ìœ¼ë¡œ setup ì‹¤í–‰
if __name__ != "__main__":
    print("ğŸŒ https://c0z0c.github.io/jupyter_hangul")
    setup()
    set_pd_root_base()
    if __is_setup_print_log:
        print('pd commit ì €ì¥ ê²½ë¡œ =', pd_root())
